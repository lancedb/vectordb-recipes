{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13cb272e",
   "metadata": {},
   "source": [
    "# Code documentation Q&A bot example with LangChain\n",
    "![picture](https://lancedb.github.io/lancedb/assets/ecosystem-illustration.png)\n",
    "\n",
    "This Q&A bot will allow you to query your own documentation easily using questions. We'll also demonstrate the use of LangChain and LanceDB using the OpenAI API.\n",
    "\n",
    "In this example we'll **Numpy 1.26** documentation, but, this could be replaced for your own docs as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1991331f-4316-417a-b693-e2f27cbe9ea7",
   "metadata": {},
   "source": [
    "### Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a49c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66638d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qq tiktoken unstructured pandas lancedb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cdcac3",
   "metadata": {},
   "source": [
    "First, let's get some setup out of the way. As we're using the OpenAI API, ensure that you've set your key (and organization if needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58ee1868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "\n",
    "# Configuring the environment variable OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# assert len(openai.models.list()[\"data\"]) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f524d3",
   "metadata": {},
   "source": [
    "\n",
    "We're going to use the power of LangChain to help us create our Q&A bot. It comes with several APIs that can make our development much easier as well as a LanceDB integration for vectorstore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e11ed32-b742-4012-a37d-af54ea2a2715",
   "metadata": {},
   "source": [
    "### Importing all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b55d22f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "import re\n",
    "import pickle\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import LanceDB\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc6d50",
   "metadata": {},
   "source": [
    "To make this easier, we've downloaded Pandas documentation and stored the raw HTML files for you to download. We'll download them and then use LangChain's HTML document readers to parse them and store them in LanceDB as a vector store, along with relevant metadata. \n",
    "By default we'll use numpy docs as it is much smaller than pandas docs, but you can replace this with your own docs as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5c79b5-7a43-403d-b456-9276cfe175b5",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7da77e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas_docs = requests.get(\"https://eto-public.s3.us-west-2.amazonaws.com/datasets/pandas_docs/pandas.documentation.zip\")\n",
    "\n",
    "numpy_docs = requests.get(\"https://numpy.org/doc/1.26/numpy-html.zip\")\n",
    "with open(\"numpy-html.zip\", \"wb\") as f:\n",
    "    f.write(numpy_docs.content)\n",
    "\n",
    "file = zipfile.ZipFile(\"numpy-html.zip\")\n",
    "file = file.extractall(path=\"numpy_docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae42496c",
   "metadata": {},
   "source": [
    "We'll create a simple **helper function** that can help to extract metadata, so we can use this downstream when we're wanting to query with filters. In this case, we want to keep the lineage of the uri or path for each document that we process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d171d062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing and loading the documentation\n",
    "\n",
    "# Next, let's pre-process and load the documentation. To make sure we don't need to do this repeatedly if we were updating code,\n",
    "# we're caching it using pickle so we can retrieve it again (this could take a few minutes to run the first time you do it).\n",
    "# We'll also add some more metadata to the docs here such as the title and version of the code:\n",
    "import re\n",
    "\n",
    "\n",
    "def get_document_title(document_list):\n",
    "    titles = []\n",
    "    for doc in document_list:\n",
    "        if \"metadata\" in doc and \"source\" in doc[\"metadata\"]:\n",
    "            m = str(doc[\"metadata\"][\"source\"])\n",
    "            title = re.findall(\"numpy_docs(.*).html\", m)\n",
    "            print(title)\n",
    "            if title:\n",
    "                titles.append(title[0])\n",
    "            else:\n",
    "                titles.append(\"\")\n",
    "        else:\n",
    "            titles.append(\"\")\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abacdc6-6f40-4d96-99bb-0e4ef18ffdfa",
   "metadata": {},
   "source": [
    "# Pre-processing and loading the documentation\n",
    "\n",
    "Next, let's pre-process and load the documentation. To make sure we don't need to do this repeatedly if we were updating code, we're caching it using pickle so we can retrieve it again (this could take a few minutes to run the first time you do it). We'll also add some more metadata to the docs here such as the title and version of the code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5609932d-21fd-4b39-a3e9-12b709358751",
   "metadata": {},
   "source": [
    "If there is some issue with nltk package, kindly try using\n",
    "```\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "```\n",
    "or try to manually install the [nltk_data](https://github.com/nltk/nltk_data/tree/gh-pages) package and unzip the **punkt tokenizer** zip and the **averaged_perceptron_tagger** zip file in the packages folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5f683a7-123b-4e9e-a60b-115bc1340a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2699it [03:03, 14.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "docs = []\n",
    "docs_path = Path(\"docs.pkl\")\n",
    "for p in tqdm(Path(\"numpy_docs\").rglob(\"*.html\")):\n",
    "    if p.is_dir():\n",
    "        continue\n",
    "    loader = UnstructuredHTMLLoader(p)\n",
    "    raw_document = loader.load()\n",
    "    # docs.append(raw_document)\n",
    "    title = get_document_title(raw_document)\n",
    "    m = {\"title\": title}\n",
    "    if raw_document:\n",
    "        raw_document[0].metadata.update(m)\n",
    "        raw_document[0].metadata[\"source\"] = str(raw_document[0].metadata[\"source\"])\n",
    "        docs.extend(raw_document)\n",
    "\n",
    "\n",
    "if docs:\n",
    "    with open(docs_path, \"wb\") as fh:\n",
    "        pickle.dump(docs, fh)\n",
    "else:\n",
    "    with open(docs_path, \"rb\") as fh:\n",
    "        docs = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c019d728-bb65-494a-b4a9-73a62bf8e155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2699"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3852dd3",
   "metadata": {},
   "source": [
    "# Generating emebeddings from our docs\n",
    "\n",
    "Now that we have our raw documents loaded, we need to pre-process them to generate embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82230563",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "documents = text_splitter.split_documents(docs)\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e68215",
   "metadata": {},
   "source": [
    "# Storing\n",
    "\n",
    "Let's connect to LanceDB Cloud so we can store our documents, It requires 0 setup !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74780a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = LanceDB(\n",
    "    embedding=embeddings,\n",
    "    uri = \"db://test\", # your remote database URI\n",
    "    api_key=\"sk_...\",\n",
    "    region=\"us-east-x-xxx\", # the cloud region you have configured \n",
    "    table_name=\"langchain_vectorstore\",  # Optional, defaults to \"vectors\"\n",
    "    mode=\"overwrite\", # Optional, defaults to \"overwrite\"\n",
    ")\n",
    "\n",
    "doc_ids = vectorstore.add_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb1dc5d",
   "metadata": {},
   "source": [
    "Now let's create our RetrievalQA chain using the LanceDB vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a5891ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(), chain_type=\"stuff\", retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d93b85",
   "metadata": {},
   "source": [
    "And thats it! We're all setup. The next step is to run some queries, let's try a few:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a5c946-5af3-45c1-862a-207ff4a34bb4",
   "metadata": {},
   "source": [
    "### Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70d88316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'tell me about the numpy library?',\n",
       " 'result': ' The NumPy library is a Python library that provides multidimensional array and matrix data structures. It is used for efficient mathematical operations on arrays and matrices, and it also offers a wide range of high-level mathematical functions. It is a fundamental library for scientific computing in Python and is used extensively in many other data science and scientific packages such as Pandas, SciPy, and OpenCV. It is known for its speed and performance, and is used by everyone from beginners to experienced researchers in various fields of science and engineering.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"tell me about the numpy library?\"\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85a0397c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"What's the current version of numpy?\",\n",
       " 'result': ' The current version of NumPy is 1.21.6, according to the context provided.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What's the current version of numpy?\"\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "923f86c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What kind of linear algebra related operations can be done in numpy?',\n",
       " 'result': ' Numpy provides a variety of linear algebra related operations, including decompositions, matrix eigenvalues, norms and other numbers, solving equations and inverting matrices, and linear algebra on several matrices at once.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What kind of linear algebra related operations can be done in numpy?\"\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8958d1b-0ad6-44d6-bca0-d81771c564a1",
   "metadata": {},
   "source": [
    "Thanks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a53efb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "511a7c77cb034b09af5465c01316a0f4bb20176d139e60e6d7915f9a637a5037"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

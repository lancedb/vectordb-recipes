# ğŸ”Search engine using SAM & CLIP

<a href="https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/Search-Engine-with-SAM-and-CLIP/main.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>  
[![Medium](https://img.shields.io/badge/Medium-12100E?style=for-the-badge&logo=medium&logoColor=white)](https://blog.lancedb.com/context-aware-chatbot-using-llama-2-lancedb-as-vector-database-4d771d95c755)


###  ğŸ« Enhance RAG: Integrate Contextual Compression and Filtering for Precision.
Follow the Colab Notebook for full code.

## Interface ğŸŒŸ

1. Load the Model from Huggingface.
2. Instantiate Contextual Compressor from Langchain.
3. Create a pipeline with different retrieving filters.
4. Get the answer using QA chain.
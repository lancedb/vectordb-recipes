{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hOraWw7fOvA"
   },
   "source": [
    "## Installing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gOk0gP_XOwKh",
    "outputId": "7d8f8ff9-09ac-4707-c2c3-f9b37b94ce07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
      "Requirement already satisfied: lancedb in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
      "Requirement already satisfied: groq in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.42.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.1+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.5)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: deprecation in /usr/local/lib/python3.10/dist-packages (from lancedb) (2.1.0)\n",
      "Requirement already satisfied: pylance==0.15.0 in /usr/local/lib/python3.10/dist-packages (from lancedb) (0.15.0)\n",
      "Requirement already satisfied: ratelimiter~=1.0 in /usr/local/lib/python3.10/dist-packages (from lancedb) (1.2.0.post0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from lancedb) (2.31.0)\n",
      "Requirement already satisfied: retry>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from lancedb) (0.9.2)\n",
      "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from lancedb) (2.8.2)\n",
      "Requirement already satisfied: attrs>=21.3.0 in /usr/local/lib/python3.10/dist-packages (from lancedb) (23.2.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lancedb) (24.1)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from lancedb) (5.4.0)\n",
      "Requirement already satisfied: overrides>=0.7 in /usr/local/lib/python3.10/dist-packages (from lancedb) (7.7.0)\n",
      "Requirement already satisfied: pyarrow>=12 in /usr/local/lib/python3.10/dist-packages (from pylance==0.15.0->lancedb) (14.0.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->lancedb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->lancedb) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->lancedb) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->lancedb) (2.0.7)\n",
      "Requirement already satisfied: decorator>=3.4.2 in /usr/local/lib/python3.10/dist-packages (from retry>=0.9.2->lancedb) (4.4.2)\n",
      "Requirement already satisfied: py<2.0.0,>=1.4.26 in /usr/local/lib/python3.10/dist-packages (from retry>=0.9.2->lancedb) (1.11.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.5.82)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers lancedb groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_XIiZEHfWcG"
   },
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25-2TzR4PAA6"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import networkx as nx\n",
    "import re\n",
    "import json\n",
    "import lancedb\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from groq import Groq\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2AB-hvUfiI_"
   },
   "source": [
    "## Getting all links at certain depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQunCX27Pebr"
   },
   "outputs": [],
   "source": [
    "def get_links(url, depth=1):  # Setting the depth to be 1\n",
    "    G = nx.DiGraph()  # Creating graph\n",
    "    G.add_node(url, depth=0)  # Adding base node\n",
    "    visited = set([url])\n",
    "    queue = [url]  # Creating a queue and visited url set.\n",
    "\n",
    "    while queue:\n",
    "        current_url = queue.pop(0)  # Getting 1st url\n",
    "        current_depth = G.nodes[current_url][\"depth\"]\n",
    "        print(current_url, current_depth)\n",
    "        if current_depth < depth:  # Checking depth\n",
    "            try:\n",
    "                response = requests.get(current_url)  # Getting Scrapped data from URL\n",
    "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                for a_tag in soup.find_all(\"a\", href=True):\n",
    "                    link = a_tag[\"href\"]\n",
    "                    if re.match(r\"^https?://\", link):  # checking for correct http link\n",
    "                        if link not in visited:\n",
    "                            G.add_node(\n",
    "                                link, depth=current_depth + 1\n",
    "                            )  # Adding sub nodes and edges\n",
    "                            G.add_edge(current_url, link)\n",
    "                            queue.append(link)  # Updating visited nodes\n",
    "                            visited.add(link)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to retrieve {current_url}: {e}\")\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urFy_9xLf1Cq"
   },
   "source": [
    "## Scrapping URL Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75W1mwlhPoYi"
   },
   "outputs": [],
   "source": [
    "def scrape_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        content = \" \".join([para.get_text() for para in paragraphs])\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gL1_vJ5Df84L"
   },
   "source": [
    "## Storing URLs with scrapped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3sebZKPSPuUG",
    "outputId": "85c0a65b-6541-400d-be41-2894127c8c7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://docs.nvidia.com/cuda/ 0\n",
      "https://developer.nvidia.com/nvidia-video-codec-sdk 1\n",
      "https://nvlabs.github.io/cub/ 1\n",
      "https://nvidia.github.io/libcudacxx/ 1\n",
      "https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html 1\n",
      "https://nvidia.github.io/cccl/thrust/ 1\n",
      "https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html 1\n",
      "https://docs.nvidia.com/deploy/cuda-compatibility/index.html 1\n",
      "https://docs.nvidia.com/cupti/index.html 1\n",
      "https://docs.nvidia.com/gpudirect-storage/index.html 1\n",
      "https://docs.nvidia.com/compute-sanitizer/index.html 1\n",
      "https://docs.nvidia.com/nsight-systems/index.html 1\n",
      "https://docs.nvidia.com/nsight-compute/index.html 1\n",
      "https://docs.nvidia.com/nsight-visual-studio-edition/index.html 1\n",
      "https://developer.nvidia.com/cuda-toolkit-archive 1\n",
      "https://www.nvidia.com/en-us/about-nvidia/privacy-policy/ 1\n",
      "https://www.nvidia.com/en-us/about-nvidia/privacy-center/ 1\n",
      "https://www.nvidia.com/en-us/preferences/start/ 1\n",
      "https://www.nvidia.com/en-us/about-nvidia/terms-of-service/ 1\n",
      "https://www.nvidia.com/en-us/about-nvidia/accessibility/ 1\n",
      "https://www.nvidia.com/en-us/about-nvidia/company-policies/ 1\n",
      "https://www.nvidia.com/en-us/product-security/ 1\n",
      "https://www.nvidia.com/en-us/contact/ 1\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://docs.nvidia.com/cuda/\"  # Base URL for scrapping data\n",
    "web_graph = get_links(base_url)  # Creating graph with URLs upto given depth\n",
    "\n",
    "content_dict = {}\n",
    "\n",
    "for url in web_graph.nodes:\n",
    "    content_dict[url] = scrape_content(\n",
    "        url\n",
    "    )  # Scrapping content and storing it in dictionary\n",
    "\n",
    "with open(\"scraped_data.json\", \"w\") as f:  # Saving scrapped data\n",
    "    json.dump(content_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Gu-FwBMgbOQ"
   },
   "source": [
    "## Chunking data and storing it's embeddings using clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_G_2-d0kP9bR"
   },
   "outputs": [],
   "source": [
    "def chunk_data(data):\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # setting model for embeddings\n",
    "    sentences = data.split(\".\")\n",
    "\n",
    "    if len(sentences) < 5:\n",
    "        sentences = data.split(\",\")\n",
    "\n",
    "    print(f\"Length of chuncked sentences : {len(sentences)}\")\n",
    "\n",
    "    if len(sentences) >= 5:\n",
    "        embeddings = model.encode(sentences)  # encoding all sentences\n",
    "\n",
    "        num_clusters = 5  # Adjust this based on your needs\n",
    "        clustering_model = KMeans(\n",
    "            n_clusters=num_clusters\n",
    "        )  # cluster diffterent sentences into different groups\n",
    "        clustering_model.fit(embeddings)\n",
    "        cluster_assignment = clustering_model.labels_\n",
    "\n",
    "        clustered_data = [\n",
    "            [] for i in range(num_clusters)\n",
    "        ]  # assigning different sentences to cluster\n",
    "        for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "            clustered_data[cluster_id - 1].append(sentences[sentence_id])\n",
    "\n",
    "        clustered_data_new = []  # joining sentences of same cluster\n",
    "        for i in clustered_data:\n",
    "            clustered_data_new.append(\".\".join(i))\n",
    "\n",
    "        embed_data = []  # storing embedding of new joined sentences\n",
    "        for i in clustered_data_new:\n",
    "            emb = model.encode(i)\n",
    "            embed_data.append(emb)\n",
    "\n",
    "        return clustered_data_new, embed_data  # returning sentences and embeddings\n",
    "    else:\n",
    "        print(\"Empty data returning\")\n",
    "        return [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNt92r9shKg1"
   },
   "source": [
    "# Storing chunks and embeddings of all URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n2MUgjUEQQx2",
    "outputId": "698566fc-d80b-4906-eee6-a1b465b016bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "URL during Chunking : https://docs.nvidia.com/cuda/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of chuncked sentences : 127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL during Chunking : https://developer.nvidia.com/nvidia-video-codec-sdk\n",
      "Length of chuncked sentences : 114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL during Chunking : https://nvlabs.github.io/cub/\n",
      "Length of chuncked sentences : 1\n",
      "Empty data returning\n",
      "URL during Chunking : https://nvidia.github.io/libcudacxx/\n",
      "Length of chuncked sentences : 1\n",
      "Empty data returning\n",
      "URL during Chunking : https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html\n",
      "Length of chuncked sentences : 322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL during Chunking : https://nvidia.github.io/cccl/thrust/\n",
      "Length of chuncked sentences : 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL during Chunking : https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html\n",
      "Length of chuncked sentences : 368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL during Chunking : https://docs.nvidia.com/deploy/cuda-compatibility/index.html\n",
      "Length of chuncked sentences : 324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL during Chunking : https://docs.nvidia.com/cupti/index.html\n",
      "Length of chuncked sentences : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL during Chunking : https://docs.nvidia.com/gpudirect-storage/index.html\n",
      "Length of chuncked sentences : 2\n",
      "Empty data returning\n",
      "URL during Chunking : https://docs.nvidia.com/compute-sanitizer/index.html\n",
      "Length of chuncked sentences : 1\n",
      "Empty data returning\n",
      "URL during Chunking : https://docs.nvidia.com/nsight-systems/index.html\n",
      "Length of chuncked sentences : 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL during Chunking : https://docs.nvidia.com/nsight-compute/index.html\n",
      "Length of chuncked sentences : 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL during Chunking : https://docs.nvidia.com/nsight-visual-studio-edition/index.html\n",
      "Length of chuncked sentences : 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL during Chunking : https://developer.nvidia.com/cuda-toolkit-archive\n",
      "Length of chuncked sentences : 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL during Chunking : https://www.nvidia.com/en-us/about-nvidia/privacy-policy/\n",
      "Length of chuncked sentences : 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL during Chunking : https://www.nvidia.com/en-us/about-nvidia/privacy-center/\n",
      "Length of chuncked sentences : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL during Chunking : https://www.nvidia.com/en-us/preferences/start/\n",
      "Length of chuncked sentences : 1\n",
      "Empty data returning\n",
      "URL during Chunking : https://www.nvidia.com/en-us/about-nvidia/terms-of-service/\n",
      "Length of chuncked sentences : 145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL during Chunking : https://www.nvidia.com/en-us/about-nvidia/accessibility/\n",
      "Length of chuncked sentences : 5\n",
      "URL during Chunking : https://www.nvidia.com/en-us/about-nvidia/company-policies/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of chuncked sentences : 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL during Chunking : https://www.nvidia.com/en-us/product-security/\n",
      "Length of chuncked sentences : 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL during Chunking : https://www.nvidia.com/en-us/contact/\n",
      "Length of chuncked sentences : 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "  with open('scraped_data.json', 'r') as f:\n",
    "      content_dict = json.load(f)             # Loading scrapped data\n",
    "\n",
    "  all_chunks = []\n",
    "  all_embeddings = []\n",
    "\n",
    "  print(len(content_dict.keys()))\n",
    "\n",
    "  for url, content in list(content_dict.items()):   # storing chunks and embeddings of all stored documents\n",
    "      print(f\"URL during Chunking : {url}\")\n",
    "      chunks, embeddings = chunk_data(content)\n",
    "      for c,e in zip(chunks,embeddings):\n",
    "          chk = c\n",
    "          chk.replace(\" \",\"\")\n",
    "          chk.replace('\\n',\"\")\n",
    "          if len(chk) < 10 :\n",
    "              continue\n",
    "          all_chunks.append(c)\n",
    "          all_embeddings.append(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5GzuSB8UUgW"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\"vector\": all_embeddings, \"docs\": all_chunks}\n",
    ")  # Storing chunks and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "J1z9uCB3UoOn",
    "outputId": "d515062d-255a-4ef2-ab4f-0993a209fa85"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 81,\n  \"fields\": [\n    {\n      \"column\": \"vector\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"docs\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 80,\n        \"samples\": [\n          \"\\n      Last updated on July 16, 2024\",\n          \" If you do not agree with the terms and conditions of the license agreement, then do not download or use the software. The user guide for CUB. The initial set of functionality in the library focuses on imaging and video processing and is widely applicable for developers in these areas. The CUPTI-API. The documentation for GPUDirect Storage. The user guide for Compute Sanitizer. Nsight Eclipse Plugins Installation Guide Nsight Eclipse Plugins Edition getting started guide The documentation for Nsight Systems. The documentation for Nsight Visual Studio Edition. This is the guide to the Profiler. \\nPrivacy Policy\\r\\n|\\r\\nManage My Privacy\\r\\n|\\r\\nDo Not Sell or Share My Data\\r\\n|\\r\\nTerms of Service\\r\\n|\\r\\nAccessibility\\r\\n|\\r\\nCorporate Policies\\r\\n|\\r\\nProduct Security\\r\\n|\\r\\nContact\\n \\u00a9 Copyright 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.\\r\\n      Last updated on Jul 1, 2024.\\r\\n      \",\n          \" \\n                            Note that prior to NVIDIA driver release R510, the combination of a (4 memory, 4\\n                              compute) and a (4 memory, 3 compute) profile was not supported. \\n                           \\n                          H100 GPUs are supported starting with CUDA 12/R525 drivers.  A100 and A30 GPUs are supported starting with CUDA 11/R450 drivers. It is also recommended to use \\n                                 the latest NVIDIA Datacenter Linux.\\n                           \\n                         \\n                              With the R470 NVIDIA datacenter drivers (470.\\n                        \\n                      \\n                           The following diagram shows the profiles supported on the NVIDIA A30:\\n                           \\n                          \\n                           The table below shows the supported profiles on the A30-24GB product.\\n                            \\n                           The following diagram shows the profiles supported on the NVIDIA A100:\\n                           \\n                          \\n                           The table below shows the supported profiles on the A100-SXM4-40GB product.\\n                            \\n                           The following diagram shows the profiles supported on the NVIDIA H100:\\n                           \\n                          \\n                           The table below shows the supported profiles on the H100 80GB product (PCIe and SXM5). \\n                           \\n                         The following diagram shows the profiles supported on the NVIDIA H200:   The table below shows the supported profiles on the H200 141GB product. Note that for brevity, some of the nvidia-smi \\n                           output in the following examples may be cropped to showcase the relevant sections of interest.h) included in the CUDA Toolkit packages \\n                           (cuda-nvml-dev-*; installed under /usr/local/cuda/include/nvml. By using the \\n                           -C option, nvidia-smi creates these instances.\\n                              \\n                            If a mixed geometry of the profiles is specified by the user, then the NVIDIA driver chooses the placement of the various\\n                              profiles.\\n                              \\n                            \\n                              Using nvidia-smi, the following CIs are now created on GI 1.\\n                           \\n                         \\n                           It can be verified that the CI devices have now been torn down on the GPU:\\n                           \\n                         \\n                           Now the GIs have to be deleted: \\n                           \\n                         \\n                           For monitoring MIG devices on including attribution of GPU metrics \\n                           (including utilization and other profiling metrics), it is recommended to use NVIDIA DCGM v3 or later. \\n                              \\n                            \\n                              Now install the NVIDIA Container Toolkit (previously known as nvidia-docker2).3 of nvidia-docker2 (or v1.1 of the \\n                              nvidia-container-toolkit package). \\n                              \\n                            \\n                              To get access to the /dev nvidia capabilities, \\n                              it is recommended to use at least v2.0 of nvidia-docker2. \\n                              Refer to the  \\n                                 NVIDIA Container Toolkit page for instructions on other Linux distributions. \\n                              \\n                            \\n                              Setup the repository and the GPG key:\\n                              \\n                            \\n                              Install the NVIDIA Container Toolkit packages (and their dependencies):\\n                              \\n                            \\n                              To run containers on specific MIG devices - whether these are GIs or specific underlying CIs, \\n                              then the NVIDIA_VISIBLE_DEVICES variable (or the --gpus option \\n                              with Docker 19.\\n                              \\n                              \\n                            \\n                              The following example shows running nvidia-smi from within a CUDA container using both formats.0 of the NVIDIA Device Plugin for Kubernetes. \\n                           \\n                         \\n                        Currently, the NVIDIA kernel driver exposes its interfaces through a few system-wide device nodes. nvidia0, nvidia1 etc.\\n                        \\n                      \\n                        Starting with CUDA 11/R450, a new abstraction known as nvidia-capabilities has been introduced. \\n                        With CAP_SYS_ADMIN privileges, you implicitly have access to all nvidia-capabilities.\\n                        \\n                      \\n                        The following sections walk through the system level interface for managing these new nvidia-capabilities, including the \\n                        steps necessary to grant and revoke access to them.\\n                        \\n                      \\n                           An example of loading the nvidia kernel module with this parameter set can be seen below:\\n                           \\n                         \\n                           The system level interface for interacting with /dev based capabilities is actually through a combination of /proc and /dev.\\n                           \\n                         \\n                           First, a new major device is now associated with nvidia-caps and can be read from the standard /proc/devices file.\\n                           \\n                         \\n                           Second, the exact same set of files exist under /proc/driver/nvidia/capabilities.\\n                           \\n                         \\n                           The standard location for these device nodes is under /dev/nvidia-caps as seen in the example below:\\n                           \\n                         \\n                           Unfortunately, these device nodes cannot be automatically created/deleted by the NVIDIA driver at the same time it creates/deletes\\n                           files underneath /proc/driver/nvidia/capabilities \\n                           (due to GPL compliance issues). Instead, a user-level program called nvidia-modprobe is provided, that can be invoked from user-space in order to do this. For example:        \\n                           \\n                         nvidia-modprobe looks at the DeviceFileMode in each capability file and creates the device node with the permissions indicated \\n                           (e.\\n                           \\n                         \\n                           Programs such as nvidia-smi will automatically invoke nvidia-modprobe (when available) to create these device nodes on your behalf. \\n                           In other scenarios it is not necessarily required to use nvidia-modprobe to create these device nodes, but it does make the process simpler. \\n                           \\n                         \\n                           If you actually want to prevent nvidia-modprobe from ever creating a particular device node on your behalf, you can do the following:\\n                           \\n                         \\n                           You will then be responsible for managing creation of the device node referenced by /proc/driver/nvidia/capabilities/mig/config going forward. through nvidia-smi) is not the device minor number.\\n                           \\n                         \\n                           The system level interface for interacting with /proc based nvidia-capabilities is rooted at \\n                           /proc/driver/nvidia/capabilities. \\n                           As shown earlier in the document, the cgroups mechanism applies to:\\n                           \\n                         \\n                           In the context of containers, a new mount namespace should be overlaid on top of the path for /proc/driver/nvidia/capabilities, and only those \\n                           capabilities a user wishes to grant to a container should be bind-mounted in.\\n                                 NVIDIA Corporation (\\u201cNVIDIA\\u201d) makes no representations or\\n                                 warranties, expressed or implied, as to the accuracy or\\n                                 completeness of the information contained in this document\\n                                 and assumes no responsibility for any errors contained\\n                                 herein. NVIDIA shall have no liability for the consequences\\n                                 or use of such information or for any infringement of\\n                                 patents or other rights of third parties that may result\\n                                 from its use. \\nNVIDIA reserves the right to make corrections, modifications,\\n                                 enhancements, improvements, and any other changes to this\\n                                 document, at any time without notice. \\nNVIDIA products are sold subject to the NVIDIA standard terms and\\n                                 conditions of sale supplied at the time of order\\n                                 acknowledgement, unless otherwise agreed in an individual\\n                                 sales agreement signed by authorized representatives of\\n                                 NVIDIA and customer (\\u201cTerms of Sale\\u201d). NVIDIA hereby\\n                                 expressly objects to applying any customer general terms and\\n                                 conditions with regards to the purchase of the NVIDIA\\n                                 product referenced in this document. \\nNVIDIA products are not designed, authorized, or warranted to be\\n                                 suitable for use in medical, military, aircraft, space, or\\n                                 life support equipment, nor in applications where failure or\\n                                 malfunction of the NVIDIA product can reasonably be expected\\n                                 to result in personal injury, death, or property or\\n                                 environmental damage. NVIDIA accepts no liability for\\n                                 inclusion and/or use of NVIDIA products in such equipment or\\n                                 applications and therefore such inclusion and/or use is at\\n                                 customer\\u2019s own risk. \\nNVIDIA makes no representation or warranty that products based on\\n                                 this document will be suitable for any specified use.\\n                                 Testing of all parameters of each product is not necessarily\\n                                 performed by NVIDIA. Weaknesses in\\n                                 customer\\u2019s product designs may affect the quality and\\n                                 reliability of the NVIDIA product and may result in\\n                                 additional or different conditions and/or requirements\\n                                 beyond those contained in this document. NVIDIA accepts no\\n                                 liability related to any default, damage, costs, or problem\\n                                 which may be based on or attributable to: (i) the use of the\\n                                 NVIDIA product in any manner that is contrary to this\\n                                 document or (ii) customer product designs. \\nNo license, either expressed or implied, is granted under any NVIDIA\\n                                 patent right, copyright, or other NVIDIA intellectual\\n                                 property right under this document. Information published by\\n                                 NVIDIA regarding third-party products or services does not\\n                                 constitute a license from NVIDIA to use such products or\\n                                 services or a warranty or endorsement thereof. Use of such\\n                                 information may require a license from a third party under\\n                                 the patents or other intellectual property rights of the\\n                                 third party, or a license from NVIDIA under the patents or\\n                                 other intellectual property rights of NVIDIA. \\nReproduction of information in this document is permissible only if\\n                                 approved in advance by NVIDIA in writing, reproduced without\\n                                 alteration and in full compliance with all applicable export\\n                                 laws and regulations, and accompanied by all associated\\n                                 conditions, limitations, and notices. \\nTHIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE\\n                                 BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER\\n                                 DOCUMENTS (TOGETHER AND SEPARATELY, \\u201cMATERIALS\\u201d) ARE BEING\\n                                 PROVIDED \\u201cAS IS.\\u201d NVIDIA MAKES NO WARRANTIES, EXPRESSED,\\n                                 IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE\\n                                 MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF\\n                                 NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A\\n                                 PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN\\n                                 NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING\\n                                 WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL,\\n                                 INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER\\n                                 CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING\\n                                 OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN\\n                                 ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding\\n                                 any damages that customer might incur for any reason\\n                                 whatsoever, NVIDIA\\u2019s aggregate and cumulative liability\\n                                 towards customer for the products described herein shall be\\n                                 limited in accordance with the Terms of Sale for the\\n                                 product. NVIDIA and the NVIDIA logo are trademarks and/or registered trademarks of NVIDIA\\n                                 Corporation in the Unites States and other countries. \\u00a9 2020-2024 NVIDIA Corporation & affiliates\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-1532e845-6340-40f9-acce-10f900f61d31\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vector</th>\n",
       "      <th>docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.025895298, -0.027912818, -0.090532914, -0....</td>\n",
       "      <td>If you do not agree with the terms and condit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.046788294, -0.05065362, -0.06595036, -0.04...</td>\n",
       "      <td>Installation Guides Programming Guides CUDA AP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.06084559, 0.010138369, -0.060803737, -0.00...</td>\n",
       "      <td>Using built-in capabilities for distributing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.083506435, 0.022876907, -0.07854397, -0.03...</td>\n",
       "      <td>\\r\\nThe toolkit includes GPU-accelerated libra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.103047416, 0.024801489, -0.06617258, -0.00...</td>\n",
       "      <td>Applications that follow the best practices f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1532e845-6340-40f9-acce-10f900f61d31')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-1532e845-6340-40f9-acce-10f900f61d31 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-1532e845-6340-40f9-acce-10f900f61d31');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-8f212bd9-ad2b-4b7c-8841-2e8d570f4b03\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8f212bd9-ad2b-4b7c-8841-2e8d570f4b03')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-8f212bd9-ad2b-4b7c-8841-2e8d570f4b03 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                              vector  \\\n",
       "0  [-0.025895298, -0.027912818, -0.090532914, -0....   \n",
       "1  [-0.046788294, -0.05065362, -0.06595036, -0.04...   \n",
       "2  [-0.06084559, 0.010138369, -0.060803737, -0.00...   \n",
       "3  [-0.083506435, 0.022876907, -0.07854397, -0.03...   \n",
       "4  [-0.103047416, 0.024801489, -0.06617258, -0.00...   \n",
       "\n",
       "                                                docs  \n",
       "0   If you do not agree with the terms and condit...  \n",
       "1  Installation Guides Programming Guides CUDA AP...  \n",
       "2   Using built-in capabilities for distributing ...  \n",
       "3  \\r\\nThe toolkit includes GPU-accelerated libra...  \n",
       "4   Applications that follow the best practices f...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLZd089_h5xj"
   },
   "source": [
    "## Creating Vector Database with lancedb and storing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOR5cVpHRqaI"
   },
   "outputs": [],
   "source": [
    "uri = \"data/url-scrap\"\n",
    "db = lancedb.connect(uri)\n",
    "\n",
    "table = db.create_table(\"url_rag\", data=df)  # Creating a vector db table from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "N313pgXkUKr1",
    "outputId": "c361e93b-95e4-4dd5-d990-f07105d5ddd3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"table\",\n  \"rows\": 81,\n  \"fields\": [\n    {\n      \"column\": \"vector\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"docs\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 80,\n        \"samples\": [\n          \"\\n      Last updated on July 16, 2024\",\n          \" If you do not agree with the terms and conditions of the license agreement, then do not download or use the software. The user guide for CUB. The initial set of functionality in the library focuses on imaging and video processing and is widely applicable for developers in these areas. The CUPTI-API. The documentation for GPUDirect Storage. The user guide for Compute Sanitizer. Nsight Eclipse Plugins Installation Guide Nsight Eclipse Plugins Edition getting started guide The documentation for Nsight Systems. The documentation for Nsight Visual Studio Edition. This is the guide to the Profiler. \\nPrivacy Policy\\r\\n|\\r\\nManage My Privacy\\r\\n|\\r\\nDo Not Sell or Share My Data\\r\\n|\\r\\nTerms of Service\\r\\n|\\r\\nAccessibility\\r\\n|\\r\\nCorporate Policies\\r\\n|\\r\\nProduct Security\\r\\n|\\r\\nContact\\n \\u00a9 Copyright 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.\\r\\n      Last updated on Jul 1, 2024.\\r\\n      \",\n          \" \\n                            Note that prior to NVIDIA driver release R510, the combination of a (4 memory, 4\\n                              compute) and a (4 memory, 3 compute) profile was not supported. \\n                           \\n                          H100 GPUs are supported starting with CUDA 12/R525 drivers.  A100 and A30 GPUs are supported starting with CUDA 11/R450 drivers. It is also recommended to use \\n                                 the latest NVIDIA Datacenter Linux.\\n                           \\n                         \\n                              With the R470 NVIDIA datacenter drivers (470.\\n                        \\n                      \\n                           The following diagram shows the profiles supported on the NVIDIA A30:\\n                           \\n                          \\n                           The table below shows the supported profiles on the A30-24GB product.\\n                            \\n                           The following diagram shows the profiles supported on the NVIDIA A100:\\n                           \\n                          \\n                           The table below shows the supported profiles on the A100-SXM4-40GB product.\\n                            \\n                           The following diagram shows the profiles supported on the NVIDIA H100:\\n                           \\n                          \\n                           The table below shows the supported profiles on the H100 80GB product (PCIe and SXM5). \\n                           \\n                         The following diagram shows the profiles supported on the NVIDIA H200:   The table below shows the supported profiles on the H200 141GB product. Note that for brevity, some of the nvidia-smi \\n                           output in the following examples may be cropped to showcase the relevant sections of interest.h) included in the CUDA Toolkit packages \\n                           (cuda-nvml-dev-*; installed under /usr/local/cuda/include/nvml. By using the \\n                           -C option, nvidia-smi creates these instances.\\n                              \\n                            If a mixed geometry of the profiles is specified by the user, then the NVIDIA driver chooses the placement of the various\\n                              profiles.\\n                              \\n                            \\n                              Using nvidia-smi, the following CIs are now created on GI 1.\\n                           \\n                         \\n                           It can be verified that the CI devices have now been torn down on the GPU:\\n                           \\n                         \\n                           Now the GIs have to be deleted: \\n                           \\n                         \\n                           For monitoring MIG devices on including attribution of GPU metrics \\n                           (including utilization and other profiling metrics), it is recommended to use NVIDIA DCGM v3 or later. \\n                              \\n                            \\n                              Now install the NVIDIA Container Toolkit (previously known as nvidia-docker2).3 of nvidia-docker2 (or v1.1 of the \\n                              nvidia-container-toolkit package). \\n                              \\n                            \\n                              To get access to the /dev nvidia capabilities, \\n                              it is recommended to use at least v2.0 of nvidia-docker2. \\n                              Refer to the  \\n                                 NVIDIA Container Toolkit page for instructions on other Linux distributions. \\n                              \\n                            \\n                              Setup the repository and the GPG key:\\n                              \\n                            \\n                              Install the NVIDIA Container Toolkit packages (and their dependencies):\\n                              \\n                            \\n                              To run containers on specific MIG devices - whether these are GIs or specific underlying CIs, \\n                              then the NVIDIA_VISIBLE_DEVICES variable (or the --gpus option \\n                              with Docker 19.\\n                              \\n                              \\n                            \\n                              The following example shows running nvidia-smi from within a CUDA container using both formats.0 of the NVIDIA Device Plugin for Kubernetes. \\n                           \\n                         \\n                        Currently, the NVIDIA kernel driver exposes its interfaces through a few system-wide device nodes. nvidia0, nvidia1 etc.\\n                        \\n                      \\n                        Starting with CUDA 11/R450, a new abstraction known as nvidia-capabilities has been introduced. \\n                        With CAP_SYS_ADMIN privileges, you implicitly have access to all nvidia-capabilities.\\n                        \\n                      \\n                        The following sections walk through the system level interface for managing these new nvidia-capabilities, including the \\n                        steps necessary to grant and revoke access to them.\\n                        \\n                      \\n                           An example of loading the nvidia kernel module with this parameter set can be seen below:\\n                           \\n                         \\n                           The system level interface for interacting with /dev based capabilities is actually through a combination of /proc and /dev.\\n                           \\n                         \\n                           First, a new major device is now associated with nvidia-caps and can be read from the standard /proc/devices file.\\n                           \\n                         \\n                           Second, the exact same set of files exist under /proc/driver/nvidia/capabilities.\\n                           \\n                         \\n                           The standard location for these device nodes is under /dev/nvidia-caps as seen in the example below:\\n                           \\n                         \\n                           Unfortunately, these device nodes cannot be automatically created/deleted by the NVIDIA driver at the same time it creates/deletes\\n                           files underneath /proc/driver/nvidia/capabilities \\n                           (due to GPL compliance issues). Instead, a user-level program called nvidia-modprobe is provided, that can be invoked from user-space in order to do this. For example:        \\n                           \\n                         nvidia-modprobe looks at the DeviceFileMode in each capability file and creates the device node with the permissions indicated \\n                           (e.\\n                           \\n                         \\n                           Programs such as nvidia-smi will automatically invoke nvidia-modprobe (when available) to create these device nodes on your behalf. \\n                           In other scenarios it is not necessarily required to use nvidia-modprobe to create these device nodes, but it does make the process simpler. \\n                           \\n                         \\n                           If you actually want to prevent nvidia-modprobe from ever creating a particular device node on your behalf, you can do the following:\\n                           \\n                         \\n                           You will then be responsible for managing creation of the device node referenced by /proc/driver/nvidia/capabilities/mig/config going forward. through nvidia-smi) is not the device minor number.\\n                           \\n                         \\n                           The system level interface for interacting with /proc based nvidia-capabilities is rooted at \\n                           /proc/driver/nvidia/capabilities. \\n                           As shown earlier in the document, the cgroups mechanism applies to:\\n                           \\n                         \\n                           In the context of containers, a new mount namespace should be overlaid on top of the path for /proc/driver/nvidia/capabilities, and only those \\n                           capabilities a user wishes to grant to a container should be bind-mounted in.\\n                                 NVIDIA Corporation (\\u201cNVIDIA\\u201d) makes no representations or\\n                                 warranties, expressed or implied, as to the accuracy or\\n                                 completeness of the information contained in this document\\n                                 and assumes no responsibility for any errors contained\\n                                 herein. NVIDIA shall have no liability for the consequences\\n                                 or use of such information or for any infringement of\\n                                 patents or other rights of third parties that may result\\n                                 from its use. \\nNVIDIA reserves the right to make corrections, modifications,\\n                                 enhancements, improvements, and any other changes to this\\n                                 document, at any time without notice. \\nNVIDIA products are sold subject to the NVIDIA standard terms and\\n                                 conditions of sale supplied at the time of order\\n                                 acknowledgement, unless otherwise agreed in an individual\\n                                 sales agreement signed by authorized representatives of\\n                                 NVIDIA and customer (\\u201cTerms of Sale\\u201d). NVIDIA hereby\\n                                 expressly objects to applying any customer general terms and\\n                                 conditions with regards to the purchase of the NVIDIA\\n                                 product referenced in this document. \\nNVIDIA products are not designed, authorized, or warranted to be\\n                                 suitable for use in medical, military, aircraft, space, or\\n                                 life support equipment, nor in applications where failure or\\n                                 malfunction of the NVIDIA product can reasonably be expected\\n                                 to result in personal injury, death, or property or\\n                                 environmental damage. NVIDIA accepts no liability for\\n                                 inclusion and/or use of NVIDIA products in such equipment or\\n                                 applications and therefore such inclusion and/or use is at\\n                                 customer\\u2019s own risk. \\nNVIDIA makes no representation or warranty that products based on\\n                                 this document will be suitable for any specified use.\\n                                 Testing of all parameters of each product is not necessarily\\n                                 performed by NVIDIA. Weaknesses in\\n                                 customer\\u2019s product designs may affect the quality and\\n                                 reliability of the NVIDIA product and may result in\\n                                 additional or different conditions and/or requirements\\n                                 beyond those contained in this document. NVIDIA accepts no\\n                                 liability related to any default, damage, costs, or problem\\n                                 which may be based on or attributable to: (i) the use of the\\n                                 NVIDIA product in any manner that is contrary to this\\n                                 document or (ii) customer product designs. \\nNo license, either expressed or implied, is granted under any NVIDIA\\n                                 patent right, copyright, or other NVIDIA intellectual\\n                                 property right under this document. Information published by\\n                                 NVIDIA regarding third-party products or services does not\\n                                 constitute a license from NVIDIA to use such products or\\n                                 services or a warranty or endorsement thereof. Use of such\\n                                 information may require a license from a third party under\\n                                 the patents or other intellectual property rights of the\\n                                 third party, or a license from NVIDIA under the patents or\\n                                 other intellectual property rights of NVIDIA. \\nReproduction of information in this document is permissible only if\\n                                 approved in advance by NVIDIA in writing, reproduced without\\n                                 alteration and in full compliance with all applicable export\\n                                 laws and regulations, and accompanied by all associated\\n                                 conditions, limitations, and notices. \\nTHIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE\\n                                 BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER\\n                                 DOCUMENTS (TOGETHER AND SEPARATELY, \\u201cMATERIALS\\u201d) ARE BEING\\n                                 PROVIDED \\u201cAS IS.\\u201d NVIDIA MAKES NO WARRANTIES, EXPRESSED,\\n                                 IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE\\n                                 MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF\\n                                 NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A\\n                                 PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN\\n                                 NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING\\n                                 WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL,\\n                                 INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER\\n                                 CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING\\n                                 OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN\\n                                 ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding\\n                                 any damages that customer might incur for any reason\\n                                 whatsoever, NVIDIA\\u2019s aggregate and cumulative liability\\n                                 towards customer for the products described herein shall be\\n                                 limited in accordance with the Terms of Sale for the\\n                                 product. NVIDIA and the NVIDIA logo are trademarks and/or registered trademarks of NVIDIA\\n                                 Corporation in the Unites States and other countries. \\u00a9 2020-2024 NVIDIA Corporation & affiliates\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-8c6964b5-636f-4be7-96a6-438fb5dcb681\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vector</th>\n",
       "      <th>docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.025895298, -0.027912818, -0.090532914, -0....</td>\n",
       "      <td>If you do not agree with the terms and condit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.046788294, -0.05065362, -0.06595036, -0.04...</td>\n",
       "      <td>Installation Guides Programming Guides CUDA AP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.06084559, 0.010138369, -0.060803737, -0.00...</td>\n",
       "      <td>Using built-in capabilities for distributing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.083506435, 0.022876907, -0.07854397, -0.03...</td>\n",
       "      <td>\\r\\nThe toolkit includes GPU-accelerated libra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.103047416, 0.024801489, -0.06617258, -0.00...</td>\n",
       "      <td>Applications that follow the best practices f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>[-0.02574309, 0.032440744, -0.021723332, 0.015...</td>\n",
       "      <td>Below is a list of published NVIDIA Security ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>[-0.05397123, -0.069211304, 0.046810895, -0.00...</td>\n",
       "      <td>Get help with your existing NVIDIA products an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>[0.12230451, 0.03236777, 0.0051685073, 5.24056...</td>\n",
       "      <td>Alabama Madison  California Palo Alto Santa C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>[0.05249632, -0.07003562, 0.010563484, 0.02913...</td>\n",
       "      <td>Find experienced, professional partners. More...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>[0.033717245, 0.0032893985, -0.0040543075, -0....</td>\n",
       "      <td>2788 San Tomas Expressway Santa Clara, CA 950...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows  2 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c6964b5-636f-4be7-96a6-438fb5dcb681')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-8c6964b5-636f-4be7-96a6-438fb5dcb681 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-8c6964b5-636f-4be7-96a6-438fb5dcb681');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-ceab7dce-11a9-410a-b46c-8f13813786a9\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ceab7dce-11a9-410a-b46c-8f13813786a9')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-ceab7dce-11a9-410a-b46c-8f13813786a9 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                               vector  \\\n",
       "0   [-0.025895298, -0.027912818, -0.090532914, -0....   \n",
       "1   [-0.046788294, -0.05065362, -0.06595036, -0.04...   \n",
       "2   [-0.06084559, 0.010138369, -0.060803737, -0.00...   \n",
       "3   [-0.083506435, 0.022876907, -0.07854397, -0.03...   \n",
       "4   [-0.103047416, 0.024801489, -0.06617258, -0.00...   \n",
       "..                                                ...   \n",
       "76  [-0.02574309, 0.032440744, -0.021723332, 0.015...   \n",
       "77  [-0.05397123, -0.069211304, 0.046810895, -0.00...   \n",
       "78  [0.12230451, 0.03236777, 0.0051685073, 5.24056...   \n",
       "79  [0.05249632, -0.07003562, 0.010563484, 0.02913...   \n",
       "80  [0.033717245, 0.0032893985, -0.0040543075, -0....   \n",
       "\n",
       "                                                 docs  \n",
       "0    If you do not agree with the terms and condit...  \n",
       "1   Installation Guides Programming Guides CUDA AP...  \n",
       "2    Using built-in capabilities for distributing ...  \n",
       "3   \\r\\nThe toolkit includes GPU-accelerated libra...  \n",
       "4    Applications that follow the best practices f...  \n",
       "..                                                ...  \n",
       "76   Below is a list of published NVIDIA Security ...  \n",
       "77  Get help with your existing NVIDIA products an...  \n",
       "78   Alabama Madison  California Palo Alto Santa C...  \n",
       "79   Find experienced, professional partners. More...  \n",
       "80   2788 San Tomas Expressway Santa Clara, CA 950...  \n",
       "\n",
       "[81 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bL5uZFGvjphh"
   },
   "source": [
    "## Searching top results related to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HEZ1-jhjUw-Z"
   },
   "outputs": [],
   "source": [
    "def hybrid_retrieval(query, table):\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    query_embedding = model.encode(query)  # Encoding query\n",
    "\n",
    "    results = (\n",
    "        table.search(query_embedding).limit(5).to_pandas()\n",
    "    )  # Getting top 5 results\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "QuMYZKRzV0m5"
   },
   "outputs": [],
   "source": [
    "query = \"What is Cuda ?\"\n",
    "res = hybrid_retrieval(query, table)\n",
    "res = list(res.docs)  # Retrieved top 5 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IFsaGx0VjXTn",
    "outputId": "8cc8dea9-3420-479e-f2aa-5832ea4863ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CUDA Compatibility CUDA Compatibility describes the use of new CUDA toolkit components on systems with older base installations. The NVIDIA CUDA Toolkit enables developers to build NVIDIA GPU accelerated compute applications for desktop computers, enterprise, and data centers to hyperscalers. It consists of the CUDA compiler toolchain including the CUDA runtime (cudart) and various CUDA libraries and tools. To build an application, a developer has to install only the CUDA Toolkit and necessary libraries required for linking. In order to run a CUDA application, the system should have a CUDA enabled GPU and an NVIDIA display driver that is compatible with the CUDA Toolkit that was used to build the application itself. Figure 1 Components of CUDA\\uf0c1 Every CUDA toolkit also ships with an NVIDIA display driver package for convenience. This driver supports all the features introduced in that version of the CUDA Toolkit. The driver package includes both the user mode CUDA driver (libcuda. Typically, upgrading a CUDA Toolkit involves upgrading both the toolkit and the driver to get the bleeding edge toolkit and driver capabilities. Figure 2 CUDA Upgrade Path\\uf0c1 But this is not always required. CUDA Compatibility guarantees allow for upgrading only certain components and that will be the focus of the rest of this document. We will see how the upgrade to a new CUDA Toolkit can be simplified to not always require a full system upgrade. From CUDA 11 onwards, applications compiled with a CUDA Toolkit release from within a CUDA major release family can run, with limited feature-set, on systems having at least the minimum required driver version as indicated below. This minimum required driver can be different from the driver packaged with the CUDA Toolkit but should belong to the same major release. Refer to the CUDA Toolkit Release Notes for the complete table. CUDA Toolkit Linux x86_64 Minimum Required Driver Version Windows Minimum Required Driver Version CUDA 12.39* CUDA 11.39 (Windows) as indicated, minor version compatibility is possible across the CUDA 11. While applications built against any of the older CUDA Toolkits always continued to function on newer drivers due to binary backward compatibility, before CUDA 11, applications built against newer CUDA Toolkit releases were not supported on older drivers without forward compatibility package. If you are using a new CUDA 10. Consequently, the minimum required driver version changed for every new CUDA Toolkit minor release until CUDA 11. Therefore, system administrators always have to upgrade drivers in order to support applications built against CUDA Toolkits from 10. CUDA Toolkit Linux x86_64 Minimum Required Driver Version Windows Minimum Required Driver Version CUDA 10.22 CUDA 10.96 CUDA 10.31 With minor version compatibility, upgrading to CUDA 11.02 that was shipped with CUDA 11.0, as shown below: Minimum required driver version guidance can be found in the CUDA Toolkit Release Notes. If either of these caveats are limiting, then a new CUDA driver from the same minor version of the toolkit that the application was built with or later is required. Limited feature set Sometimes features introduced in a CUDA Toolkit version may actually span both the toolkit and the driver. Refer to the CUDA Compatibility Developers Guide for more details. PTX Developers should refer to the CUDA Compatibility Developers Guide and PTX programming guide in the CUDA C++ Programming Guide for details on this limitation. As described, applications that directly rely only on the CUDA runtime can be deployed in the following two scenarios: CUDA driver thats installed on the system is newer than the runtime.\\r\\nCUDA runtime is newer than the CUDA driver on the system but they are from the same major release of CUDA Toolkit. For example, each cuDNN version requires a certain version of cuBLAS. Figure 3 NVRTC supports minor version compatibility from CUDA 11. To support such scenarios, CUDA introduced a Forward Compatibility Upgrade path in CUDA 10. Its mainly intended to support applications built on newer CUDA Toolkits to run on systems installed with an older NVIDIA Linux GPU driver from different major release families. This new forward-compatible upgrade path requires the use of a special package called CUDA compat package. The CUDA compat package is available in the local installers or the CUDA network repositories provided by NVIDIA as cuda-compat-12.5 it will be found in /usr/local/cuda-12. The cuda-compat package consists of the following files: libcuda.* - the CUDA Driver libnvidia-nvvm.* - JIT LTO ( CUDA 11.* -GPU debugging support for CUDA Driver (CUDA 11.8 and later only) These files should be kept together as the CUDA driver is dependent on the libnvidia-ptxjitcompiler. Example: CUDA Compatibility is installed and the application can now run successfully as shown below. In this example, the user sets LD_LIBRARY_PATH to include the files installed by the cuda-compat-12-1 package. Check the files installed under /usr/local/cuda/compat: The user can set LD_LIBRARY_PATH to include the files installed before running the CUDA 12.1 application: The cuda-compat package files can also be extracted from the appropriate datacenter driver runfile installers (. Copy the four CUDA compatibility upgrade files, listed at the start of this section, into a user- or root-created directory. Note Symlinks under /usr/local/cuda/compat need to be created manually when using the runfile installer. CUDA forward compat packages should be used only in the following situations when forward compatibility is required across major releases. The CUDA compat package is named after the highest toolkit that it can support.5 application support, please install the cuda-compat package for 12.x toolkits, then the cuda-compat package is not required in most cases. Unlike the minor-version compatibility that is defined between CUDA runtime and CUDA driver, forward compatibility is defined between the kernel driver and the CUDA driver, and hence such restrictions do not apply. NVIDIA Kernel Mode Driver - Production Branch CUDA Forward Compatible Upgrade 470.02+ (CUDA 11.02+ (CUDA 12.03+ (CUDA 12.06+ (CUDA 12.14+ (CUDA 12.02+ (CUDA 12.xx) are not supported targets for CUDA Forward Compatibility. Examples of how to read this table: The CUDA 12-4 compat package is Compatible with driver versions 470, 535. The CUDA 12-3 release is not-compatible (X) with driver version 550 as it was released prior to the driver. There are specific features in the CUDA driver that require kernel-mode support and will only work with a newer kernel mode driver. CUDA Forward Compatible Upgrade CUDA - OpenGL/Vulkan Interop cuMemMap* set of functionalities System Base Installation: 525 (>=. In addition to the CUDA driver and certain compiler components, there are other drivers in the system installation stack (for example, OpenCL) that remain on the old version. The forward-compatible upgrade path is for CUDA only. A well-written application should use following error codes to determine if CUDA Forward Compatible Upgrade is supported. CUDA_ERROR_SYSTEM_DRIVER_MISMATCH = 803. This error indicates that there is a mismatch between the versions of the display driver and the CUDA driver. CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE = 804. This error indicates that the system was upgraded to run with forward compatibility but the visible hardware detected by CUDA does not support this configuration. There are two models of deployment for the CUDA compat package. The user can set LD_LIBRARY_PATH to include the files installed before running the CUDA 11. In the cases where the module script cannot use CUDA compatible upgrade, a fallback path to the default systems installed CUDA driver can provide a more consistent experience and this can be achieved using RPATH. The CUDA driver maintains backward compatibility to continue support of applications built on older toolkits. Using a compatible minor driver version, applications build on CUDA Toolkit 11 and newer are supported on any driver from within the corresponding major release. Using the CUDA Forward Compatibility package, system administrators can run applications built using a newer toolkit even when an older driver that does not satisfy the minimum required driver version is installed on the system. This forward compatibility allows the CUDA deployments in data centers and enterprises to benefit from the faster release cadence and the latest features and performance of CUDA Toolkit. CUDA compatibility helps users by: Faster upgrades to the latest CUDA releases: Enterprises or data centers with NVIDIA GPUs have complex workflows and require advance planning for NVIDIA driver rollouts. Not having to update the driver for newer CUDA releases can mean that new versions of the software can be made available faster to users without any delays. Faster upgrades of the CUDA libraries: Users can upgrade to the latest software libraries and applications built on top of CUDA (for example, math libraries or deep learning frameworks) without an upgrade to the entire CUDA Toolkit or driver. This is possible as these libraries and frameworks do not have a direct dependency on the CUDA runtime, compiler or driver. This section includes some FAQs related to CUDA compatibility. What is the difference between CUDA forward compatible upgrade and CUDA minor version compatibility? When should users use these features? Area CUDA Forward Compatible Upgrade CUDA Minor Version Compatibility Compatibility Across older drivers from different major release versions of CUDA. Across minor release versions of CUDA only. Between kernel driver and user mode CUDA driver. Between libraries or runtimes that link to the CUDA driver. When to use If you cannot upgrade the kernel driver but need to use the latest CUDA Toolkit. GPUs supported 11. All GPU products supported OS distributions supported Linux only Windows, Linux Features supported Some features such as (CUDA-GL interop, Power 9 ATS, cuMemMap APIs) are not supported. All existing CUDA features (from older minor releases) work. CUDA releases supported All CUDA releases supported through the lifetime of the datacenter driver branch. For example,\\r\\nR418 (CUDA 10.1) EOLs in March 2022 - so all CUDA versions released (including major releases) during this timeframe are supported.\\r\\nCompatibility is not supported across major CUDA releases. Users can also set up LD_LIBRARY_PATH with the new libraries from the cuda-compat-* package. Note For mobile compatibility information, see CUDA Upgradable Package for Jetson. Does CUDA forward compatible upgrades work intra-branch? Users can upgrade the kernel mode driver within the same branch. Sometimes this may require updating the cuda-compat* package. Which GPUs are supported by the driver? The CUDA compatible upgrade is meant to ease the management of large production systems for enterprise customers.\\r\\nIts important to note that HW support is defined by the kernel mode driver and as such, newer CUDA drivers on their own will not enable new HW support.5 - current latest Refer to CUDA Driver Lifecycle to find the latest supported driver. If we build our CUDA application using CUDA 11.0, can it continue to be used with newer NVIDIA drivers (such as CUDA 11.)? Or is it only the other way around? Drivers have always been backwards compatible with CUDA. This means that a CUDA 11. CUDA applications typically statically include all the libraries (for example cudart, CUDA math libraries such as cuBLAS, cuFFT) they need, so they should work on new drivers or CUDA Toolkit installations. In other words, since CUDA is backward compatible, existing CUDA applications can continue to be used with newer CUDA versions. What is the minimum CUDA 11.x driver that supports the CUDA minor version compatibility? The minimum driver version required is 450. What about new features introduced in minor releases of CUDA? How does a developer build an application using newer CUDA Toolkits (e.x) work on a system with a CUDA 11.0 driver (R450)? By using new CUDA versions, users can benefit from new CUDA programming model APIs, compiler optimizations and math library features. A subset of CUDA APIs dont need a new driver and they can all be used without any driver dependencies. To use other CUDA APIs introduced in a minor release (that require a new driver), one would have to implement fallbacks or fail gracefully. This situation is not different from what is available today where developers use macros to compile out features based on CUDA versions. Users should refer to the CUDA headers and documentation for new CUDA APIs introduced in a release. Does CUDA compatibility work with containers? Yes. CUDA minor version compatibility and CUDA forward compatible upgrade both work when using either NGC Deep Learning Framework containers or using containers that are based on the official CUDA base images. The images include the CUDA compatible upgrade libraries and the NVIDIA Container Toolkit (nvidia-docker2) has logic to correctly load the required libraries',\n",
       " 'Installation Guides Programming Guides CUDA API References PTX Compiler API References Miscellaneous Tools White Papers Application Notes Compiler SDK Develop, Optimize and Deploy GPU-Accelerated Apps The NVIDIA CUDA Toolkit provides a development environment for creating high performance GPU-accelerated\\r\\napplications. With the CUDA Toolkit, you can develop, optimize, and deploy your applications on GPU-accelerated\\r\\nembedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers. The Release Notes for the CUDA Toolkit. The list of CUDA features by release. The CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools (Visual Studio Edition), and the associated documentation on CUDA APIs, programming model and development tools. This guide provides the minimal first-steps instructions for installation and verifying CUDA on a standard system. This guide discusses how to install and check for correct operation of the CUDA Development Tools on Microsoft Windows systems. This guide discusses how to install and check for correct operation of the CUDA Development Tools on GNU/Linux systems. This guide provides a detailed discussion of the CUDA programming model and programming interface. The appendices include a list of all CUDA-enabled devices, detailed description of all extensions to the C++ language, listings of supported mathematical functions, C++ features supported in host and device code, details on texture fetching, technical specifications of various devices, and concludes by introducing the low-level driver API. This document shows how to write PTX that is ABI-compliant and interoperable with other CUDA code. This document shows how to inline PTX (parallel thread execution) assembly language statements into CUDA code. The CUDA math API. The cuBLAS library is an implementation of BLAS (Basic Linear Algebra Subprograms) on top of the NVIDIA CUDA runtime. The cuDLA API. The API reference for libcu++, the CUDA C++ standard library. The cuRAND library user guide. The cuSPARSE library user guide. NVRTC is a runtime compilation library for CUDA C++. It accepts CUDA C++ source code in character string form and creates handles that can be used to obtain the PTX. The PTX string generated by NVRTC can be loaded by cuModuleLoadData and cuModuleLoadDataEx, and linked with other modules by cuLinkAddData of the CUDA Driver API. The cuSOLVER library user guide. This guide shows how to compile a PTX program into GPU assembly code using APIs provided by the static PTX Compiler library. This document describes the demo applications shipped with the CUDA Demo Suite. This guide is intended to help users get started with using NVIDIA CUDA on Windows Subsystem for Linux (WSL 2). The guide covers installation and running CUDA applications and containers in this environment. This document describes CUDA Compatibility, including CUDA Enhanced Compatibility and CUDA Forward Compatible Upgrade. The CUDA Profiling Tools Interface (CUPTI) enables the creation of profiling and tracing tools that target CUDA applications. The CUDA debugger API. vGPUs that support CUDA. This is a reference document for nvcc, the CUDA compiler driver. CUDA-GDB is an extension to the x86-64 port of GDB, the GNU Project debugger. The application notes for cuobjdump, nvdisasm, and nvprune. High-level language front-ends, like the CUDA C compiler front-end, can generate NVVM IR',\n",
       " 'The API reference for CUPTI, the CUDA Profiling Tools Interface',\n",
       " 'CUDA Debugger Reference Release Information Copyright and License Notices NVIDIA Nsight Visual Studio Edition is an application development environment which brings GPU computing into Microsoft Visual Studio. See the latest features and updates for this version of NVIDIA Nsight Visual Studio Edition. This chapter walks you through the system requirements for NVIDIA Nsight Visual Studio Edition, and the steps youll need to install and get started using the software. Additional resources for learning more about working with NVIDIA Nsight Visual Studio Edition. Find documentation for previous versions of NVIDIA Nsight Visual Studio Edition. This document is the End User License Agreement (EULA) for NVIDIA Nsight Visual Studio Edition. This document contains specific license terms and conditions for NVIDIA Nsight Visual Studio Edition',\n",
       " 'Previous releases of the CUDA Toolkit, GPU Computing SDK, documentation and developer drivers can be found using the links below. Latest ReleaseCUDA Toolkit 12.1  (July 2024), Versioned Online Documentation Archived Releases  Learn more about the latest CUDA Toolkit and the CUDA Tools and Library Ecosystem']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNbIP3DLj5fk"
   },
   "source": [
    "## Using llama-3.1-70b for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "AHVAGDloV6xm"
   },
   "outputs": [],
   "source": [
    "# Calling llama 3.1 from groq to fetch the results using given query and documents.\n",
    "client = Groq(\n",
    "    api_key=\"gsk_Your_Grok_Key\",\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"You have been given a query {query}. Based on the given top 5 document chunks {res} return the answer.\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nSWnQ4uYXfce",
    "outputId": "22f53206-e090-40c5-9ad2-e7e3bf4c3639"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('CUDA stands for Compute Unified Device Architecture. It is a development environment created by NVIDIA for creating '\n",
      " 'high-performance applications that can run on NVIDIA GPUs. The CUDA Toolkit provides a set of tools, libraries, and '\n",
      " 'programming models for developers to build and optimize applications that can take advantage of the massively '\n",
      " 'parallel processing capabilities of NVIDIA GPUs.')\n"
     ]
    }
   ],
   "source": [
    "out = chat_completion.choices[0].message.content\n",
    "pprint(out, width=120)  # Output of the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6VRkJOYkkJQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

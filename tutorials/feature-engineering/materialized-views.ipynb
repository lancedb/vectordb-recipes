{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the tutorial, we'll create materialized views: precomputed queries stored as physical tables. Some reasons you might want materialized views:\n",
    "\n",
    "- Training on data only from a certain country\n",
    "- Building different models on shirts, pants, dresses, and coats\n",
    "- Using only a subset of data to test your training pipeline, then swapping in the whole dataset later\n",
    "\n",
    "LanceDB makes materialized views super easy to create and keep updated; let's jump in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade geneva pandas \"ray[default]\" kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion\n",
    "\n",
    "We'll use the same fashion-product-images dataset as in the Feature Engineering 101 tutorial. All the data loading is the same as that tutorial, but we'll also grab the `articleType`, and we'll make **one important change** while creating the Lance table:\n",
    "\n",
    "## Enable Stable Row IDs\n",
    "\n",
    "In classic Lance tables, compaction may cause row IDs to change. This causes problems for materialized views, which are based on the row IDs. So, when creating our table, we will use the `new_table_enable_stable_row_ids` option:\n",
    "\n",
    "    \n",
    "    db.create_table(\n",
    "        name='my_table',\n",
    "        data=my_data,\n",
    "        storage_options={'new_table_enable_stable_row_ids': 'true'}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!sudo rm -r db fashion-dataset # Uncomment and run this to delete the dataset if it already exists\n",
    "\n",
    "# Download the dataset if it doesn't exist\n",
    "!test -d fashion-dataset && test -n \"$(ls -A fashion-dataset 2>/dev/null)\" && \\\n",
    "  echo \"Dataset already exists, skipping download\" || \\\n",
    "    (curl -L -o fashion-product-images-small.zip https://www.kaggle.com/api/v1/datasets/download/paramaggarwal/fashion-product-images-small \\\n",
    "    && unzip -q fashion-product-images-small.zip -d fashion-dataset/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geneva\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_SIZE = 1000\n",
    "CONCURRENCY = 4 # Increase this if you have more CPUs available and want it to run faster.\n",
    "CHECKPOINT_SIZE = 300\n",
    "\n",
    "IMG_DIR = Path(\"fashion-dataset/images\")\n",
    "STYLE_CSV = Path(\"fashion-dataset/styles.csv\")\n",
    "DB_PATH = \"./db\"\n",
    "TABLE_NAME = \"products\"\n",
    "INSERT_FRAG_SIZE = min(1000, DATASET_SIZE / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(STYLE_CSV, on_bad_lines='skip')\n",
    "df = df.dropna(subset=[\"id\", \"productDisplayName\"])\n",
    "df = df.drop_duplicates(subset=[\"id\"], keep=\"first\")\n",
    "\n",
    "def generate_rows(df, img_dir, start = 0, end = DATASET_SIZE):\n",
    "    for _, row in df.iloc[start:end].iterrows():\n",
    "        img_path = img_dir / f\"{row['id']}.jpg\"\n",
    "        if not img_path.exists():\n",
    "            continue\n",
    "        with open(img_path, \"rb\") as f:\n",
    "            yield {\n",
    "                \"id\": int(row[\"id\"]),\n",
    "                \"description\": row[\"productDisplayName\"],\n",
    "                \"articleType\": row[\"articleType\"],\n",
    "                \"color\": row[\"baseColour\"],\n",
    "                \"image_bytes\": f.read()\n",
    "            }\n",
    "\n",
    "db = geneva.connect(DB_PATH)\n",
    "\n",
    "# Drop the table if it already exists so we can recreate it\n",
    "try:\n",
    "    table = db.drop_table(TABLE_NAME)\n",
    "except ValueError as e:\n",
    "    pass\n",
    "    \n",
    "data_stream = generate_rows(df, IMG_DIR)\n",
    "table = None\n",
    "\n",
    "rows = []\n",
    "for row in data_stream:\n",
    "    rows.append(row)\n",
    "    if len(rows) == INSERT_FRAG_SIZE:\n",
    "        if table:\n",
    "            table.add(rows)\n",
    "        else:\n",
    "            # Important! Make sure to set \"new_table_enable_stable_row_ids\" to \"true\" to use materialized views!\n",
    "            table = db.create_table(TABLE_NAME, data=rows, storage_options={'new_table_enable_stable_row_ids': 'true'})\n",
    "        rows = []\n",
    "if rows:\n",
    "    table.add(rows)\n",
    "    \n",
    "len(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Materialized Views\n",
    "\n",
    "Imagine that we work at a clothing company, and `table` represents all the styles of clothing that we sell. Then let's say that we're building a model that focuses on shirts, so we only care about shirts. We could run a query like this every time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table.search().where(\"articleType = 'Shirts'\").to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is unwieldy, and could become expensive if the original table got too big. Instead, we'll create a materialized view so we only have what we need. First we create the view, then we refresh it so it has current data, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import ray\n",
    "    ray.shutdown()\n",
    "    db.drop_table(\"shirts\")\n",
    "except ValueError as e:\n",
    "    pass\n",
    "shirts = table.search().where(\"articleType = 'Shirts'\").create_materialized_view(db, \"shirts\")\n",
    "with db.local_ray_context():\n",
    "    shirts.refresh()\n",
    "shirts.to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the 101 demo, we are using `db.local_ray_context()` to say \"run on a Ray instance on this computer.\" It will be much slower than running on multiple remote machines, but will simplify setup for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating our materialized view\n",
    "\n",
    "The real value of materialized views is the ability to \"view\" the source table - to reflect changes in it! Let's assume that our source table had 500 more clothing items added to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table.add(list(generate_rows(df, IMG_DIR, start = DATASET_SIZE, end = DATASET_SIZE + 500)))\n",
    "table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to see all the new Shirts reflected in the `shirts` table, we can simply `refresh()` our materialized view. This will pick up all the new Shirts from the original `table`. Importantly, it will only process these 500 new rows! For our demo, that's not a big difference, but imagine a table with 10 years' worth of data that adds new data every day: only processing the new day's data will take about 0.02% of the time compared to reprocessing the whole table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with db.local_ray_context():\n",
    "    shirts.refresh()\n",
    "shirts.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom columns\n",
    "\n",
    "You can customize your materialized views by renaming columns and using sql expressions ([Datafusion dialect](https://datafusion.apache.org/user-guide/sql/index.html)) to create new columns. Say we want to find all Jeans, but our department uses \"clothingType\" instead of \"articleType\", and we want a simple flag to separate out blue jeans vs other jeans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    db.drop_table(\"jeans\")\n",
    "except ValueError as e:\n",
    "    pass\n",
    "jeans = table.search().where(\"articleType = 'Jeans'\")\\\n",
    "    .select({\"description\": \"description\",\n",
    "             \"color\": \"color\",\n",
    "             \"clothingType\": \"articleType\",\n",
    "             \"image\": \"image_bytes\",\n",
    "             \"isBlue\": \"color == 'Blue'\"})\\\n",
    "    .create_materialized_view(db, \"jeans\")\n",
    "with db.local_ray_context():\n",
    "    jeans.refresh()\n",
    "jeans.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "This short demo shows the powerful views you can create very simply with LanceDB Materialized Views. For more, check out our [docs](https://docs.lancedb.com/geneva/jobs/materialized-views). In the next section, we'll go back to working with backfills, focusing on connecting to remote Ray clusters to do feature engineering at production scale."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

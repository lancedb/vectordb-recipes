{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering with LanceDB and Geneva\n",
    "\n",
    "This notebook will focus on the crucial process of feature engineering. We'll start with a raw dataset of fashion products, ingest it in LanceDB, then enrich our data with meaningful features that we could use to build a search engine or train a model.\n",
    "\n",
    "We will cover the following steps:\n",
    "1. **Data Ingestion**: Downloading a fashion dataset and loading it into a LanceDB table.\n",
    "2. **Declarative Feature Engineering**: Using Geneva to define and compute features on-the-fly.\n",
    "3. **Embedding Generation**: Creating vector embeddings for both images and text to enable semantic search.\n",
    "4. **Updating**: Adding more raw data to our table and rerunning our backfills on only the new data.\n",
    "\n",
    "## Note about Colab\n",
    "\n",
    "This notebook runs on Google Colab, even the free tier, but it will be slow, because it has to start a local Ray cluster and execute multiple ML models on its workers. We recommend downloading this notebook and running it locally. But if you do run on Colab, we recommend:\n",
    "- using a GPU instance (Runtime -> Change runtime type)\n",
    "- running on only 100 rows\n",
    "- not drawing conclusions about speed from this notebook. This notebook is meant as a demo of the basic workflow of feature engineering with LanceDB, not a benchmark or speed demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.12 environment at: /Users/dantasse/src/vectordb-recipes/venv\u001b[0m\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dantasse/src/vectordb-recipes/venv/lib/python3.12/site-packages/lancedb/__init__.py:251: UserWarning: lance is not fork-safe. If you are using multiprocessing, use spawn instead.\n",
      "  warnings.warn(\n",
      "/Users/dantasse/src/vectordb-recipes/venv/lib/python3.12/site-packages/lance/__init__.py:292: UserWarning: lance is not fork-safe. If you are using multiprocessing, use spawn or forkserver instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m117 packages\u001b[0m \u001b[2min 265ms\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m4 packages\u001b[0m \u001b[2min 0.91ms\u001b[0m\u001b[0m                                            \n",
      "\u001b[2mUninstalled \u001b[1m4 packages\u001b[0m \u001b[2min 157ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m4 packages\u001b[0m \u001b[2min 55ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==5.29.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.33.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.19.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.44.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.3.0\u001b[0m\n",
      "\u001b[2mUsing Python 3.12.12 environment at: /Users/dantasse/src/vectordb-recipes/venv\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m2 packages\u001b[0m \u001b[2min 48ms\u001b[0m\u001b[0m                                          \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 0.36ms\u001b[0m\u001b[0m                                            \n",
      "\u001b[2mUninstalled \u001b[1m2 packages\u001b[0m \u001b[2min 127ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 19ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.4.0\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.16.3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install --upgrade geneva lancedb kubernetes \"ray[default]\" rerankers pandas torch torchvision open-clip-torch\n",
    "# Pin transformers to a compatible version for BLIP models (needs >=4.40.0 for Unpack import)\n",
    "# If you encounter \"ImportError: cannot import name 'Unpack'\", ensure transformers>=4.40.0\n",
    "!uv pip install \"transformers>=4.40.0,<5.0.0\"\n",
    "!uv pip install pillow\n",
    "# Pin protobuf to avoid MessageFactory.GetPrototype AttributeError (removed in protobuf 6.30.0+)\n",
    "!uv pip install \"protobuf<6.30.0\"\n",
    "# working around a quirk on Colab:\n",
    "!uv pip install --force-reinstall numpy scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion\n",
    "\n",
    "First, let's download our dataset. We're using a small version of the Fashion Product Images dataset from Kaggle. This dataset contains images and metadata for a variety of fashion products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "#!sudo rm -r db fashion-dataset # Uncomment and run this to delete the dataset if it already exists\n",
    "\n",
    "# Download the dataset if it doesn't exist\n",
    "!test -d fashion-dataset && test -n \"$(ls -A fashion-dataset 2>/dev/null)\" && \\\n",
    "  echo \"Dataset already exists, skipping download\" || \\\n",
    "    (curl -L -o fashion-product-images-small.zip https://www.kaggle.com/api/v1/datasets/download/paramaggarwal/fashion-product-images-small \\\n",
    "    && unzip -q fashion-product-images-small.zip -d fashion-dataset/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Scale based on your environment\n",
    "\n",
    "This tutorial uses Ray locally to build features, which means the scale of concurrent jobs will be limited to the system you're working on. These parameters are good defaults, but feel free to adjust them if you'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Especially if on Colab, start with just 100 rows for testing.\n",
    "DATASET_SIZE = 100\n",
    "# Increase this if you're running locally, you have more CPUs available and want it to run faster.\n",
    "CONCURRENCY = 4\n",
    "CHECKPOINT_SIZE = min(300, DATASET_SIZE / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import geneva\n",
    "from geneva import udf\n",
    "import lancedb\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "import open_clip\n",
    "from typing import Callable\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "IMG_DIR = Path(\"fashion-dataset/images\")\n",
    "STYLE_CSV = Path(\"fashion-dataset/styles.csv\")\n",
    "DB_PATH = \"./db\"\n",
    "TABLE_NAME = \"products\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the data into a LanceDB table. We'll read the CSV file with the product metadata, and for each product, we'll also load the corresponding image from the `images` directory. We'll then create a LanceDB table and add the data to it in batches. LanceDB can store objects(images in this case) along with vector embeddings and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STYLE_CSV has info about the clothes: IDs, descriptions, and image paths\n",
    "# Images themselves are stored in IMG_DIR; generate_rows will combine them with the metadata\n",
    "# in STYLE_CSV and load them all in to our LanceDB table.\n",
    "df = pd.read_csv(STYLE_CSV, on_bad_lines='skip')\n",
    "df = df.dropna(subset=[\"id\", \"productDisplayName\"])\n",
    "df = df.drop_duplicates(subset=[\"id\"], keep=\"first\")\n",
    "\n",
    "def generate_rows(df, img_dir, start=0, end=DATASET_SIZE):\n",
    "    for _, row in df.iloc[start:end].iterrows():\n",
    "        img_path = img_dir / f\"{row['id']}.jpg\"\n",
    "        if not img_path.exists():\n",
    "            continue\n",
    "        with open(img_path, \"rb\") as f:\n",
    "            yield {\n",
    "                \"id\": int(row[\"id\"]),\n",
    "                \"description\": row[\"productDisplayName\"],\n",
    "                \"image_bytes\": f.read()\n",
    "            }\n",
    "\n",
    "\n",
    "db = lancedb.connect(DB_PATH)\n",
    "# Drop the table if it already exists so we can recreate it\n",
    "try:\n",
    "    table = db.drop_table(TABLE_NAME)\n",
    "except ValueError as e:\n",
    "    pass\n",
    "    \n",
    "data_stream = generate_rows(df, IMG_DIR)\n",
    "table = None\n",
    "\n",
    "# Create the table and load rows in in batches\n",
    "rows = []\n",
    "for row in data_stream:\n",
    "    rows.append(row)\n",
    "    if len(rows) == min(1000, DATASET_SIZE / 2):\n",
    "        if table:\n",
    "            table.add(rows)\n",
    "        else:\n",
    "            # You have to provide schema or some data to create the table. Here we create the \n",
    "            # table with the first batch of data for simplicity.\n",
    "            table = db.create_table(TABLE_NAME, data=rows)\n",
    "        rows = []\n",
    "if rows:\n",
    "    table.add(rows)\n",
    "    \n",
    "len(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering with Geneva\n",
    "\n",
    "Now that we have our data in a LanceDB table, we can start engineering features. We'll use Geneva to create new features for our products. \n",
    "\n",
    "### Defining geneva UDF\n",
    "\n",
    "Geneva uses Python User Defined Functions (UDFs) to define features as columns in a Lance dataset. Adding a feature is straightforward:\n",
    "\n",
    "1. Prototype your Python function in your favorite environment.\n",
    "2. Wrap the function with small UDF decorator.\n",
    "3. Register the UDF as a virtual column using Table.add_columns().\n",
    "4. Trigger a backfill operation\n",
    "\n",
    "UDFs can work on one row or a batch at a time, and can be stateful (e.g. some work is done to set up a model the first time, and future runs use the same model) or stateless. [Read more about geneva UDFs here.](https://docs.lancedb.com/geneva/udfs)\n",
    "\n",
    "### Simple Feature Extraction\n",
    "\n",
    "Let's start with a simple feature: extracting color tags from the product description. We'll define a User-Defined Function (UDF) that takes the product description as input and returns a comma-separated string of colors found in the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = geneva.connect(DB_PATH)\n",
    "table = db.open_table(TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@udf\n",
    "def color_tags(description: str)-> str:\n",
    "    colors = [\"black\", \"white\", \"red\", \"blue\", \"green\", \"yellow\", \"pink\", \"brown\", \"grey\", \"silver\"]\n",
    "    return \", \".join([c for c in colors if c in description.lower()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Computed Column\n",
    "\n",
    "Now that we've defined our feature-generating UDF, we can add it to our table as a computed column. Computed columns are computed on-the-fly when you perform a backfill operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:geneva.table:Adding column: udf={'color_tags': UDF(func=<function color_tags at 0x419233d80>, name='color_tags', cuda=False, num_cpus=1.0, num_gpus=0.0, memory=None, batch_size=None, checkpoint_size=None, task_size=None, error_handling=None, input_columns=['description'], data_type=DataType(string), version='3776862a65c8823be3d91c0e69a4ea6f', _checkpoint_key_override=None, field_metadata={})}\n"
     ]
    }
   ],
   "source": [
    "table.add_columns({\n",
    "    \"color_tags\": color_tags,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the table schema to see our newly registered UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id: int64\n",
       "description: string\n",
       "image_bytes: binary\n",
       "color_tags: string\n",
       "  -- field metadata --\n",
       "  virtual_column.udf_backend: 'DockerUDFSpecV1'\n",
       "  virtual_column.udf: '_udfs/cfbbb9dcbfc95e5b11045f473079dbeac427eedfee72' + 20\n",
       "  virtual_column.platform.arch: 'arm64'\n",
       "  virtual_column.udf_inputs: '[\"description\"]'\n",
       "  virtual_column.platform.python_version: '3.12.12'\n",
       "  virtual_column.platform.system: 'Darwin'\n",
       "  virtual_column.udf_name: 'color_tags'\n",
       "  virtual_column: 'true'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backfilling Features\n",
    "\n",
    "Triggering backfill creates a distributed job to run the UDF and populate the column values in your LanceDB table. The Geneva framework simplifies several aspects of distributed execution.\n",
    "\n",
    "Environment management: Geneva automatically packages and deploys your Python execution environment to worker nodes. This ensures that distributed execution occurs in the same environment and depedencies as your prototype.\n",
    "\n",
    "Checkpoints: Each batch of UDF execution is checkpointed so that partial results are not lost in case of job failures. Jobs can resume and avoid most of the expense of having to recalculate values.\n",
    "\n",
    "`backfill()` accepts various params to customise scale of your workload, here we'll use:\n",
    "\n",
    "* `checkpoint_size` - the number of rows that are processed before writing a checkpoint\n",
    "* `concurrency` - how many nodes are used for parallelization\n",
    "\n",
    "Here, we're using geneva locally, so we won't set up a Ray cluster, but you can also use the same setup and run distributed jobs remotely on Ray clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d80bce845144169c97ffe95644f333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c54c3b8b5f471ab2f7faef7f4c7e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[products - color_tags (2 fragments)] Rows checkpointed:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b8535260e6415c9131669cb32ee353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[products - color_tags (2 fragments)] Rows ready for commit:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe172ef7bd154a30acb3eabadcc8a1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[products - color_tags (2 fragments)] Rows committed (every 64 fragments):   0%|          | 0/100 [00:00<?, ?i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m \u001b[90m[\u001b[0m2026-01-09T16:29:14Z \u001b[33mWARN \u001b[0m lance::dataset::transaction\u001b[90m]\u001b[0m Building manifest with DataReplacement operation. This operation is not stable yet, please use with caution.\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m Final metric reconciliation timed out; metrics will still be correct on the next tracker flush if the actor stays alive\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'85a6a2e8-757e-428d-bc7c-0d74819199ed'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.backfill(\"color_tags\", checkpoint_size=CHECKPOINT_SIZE, concurrency=CONCURRENCY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our enriched data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>image_bytes</th>\n",
       "      <th>color_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15970</td>\n",
       "      <td>Turtle Check Men Navy Blue Shirt</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39386</td>\n",
       "      <td>Peter England Men Party Blue Jeans</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44984</td>\n",
       "      <td>Maxima Women White Dial Watch</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10268</td>\n",
       "      <td>Clarks Men Hang Work Leather Black Formal Shoes</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                      description  \\\n",
       "0  15970                 Turtle Check Men Navy Blue Shirt   \n",
       "1  39386               Peter England Men Party Blue Jeans   \n",
       "2  44984                    Maxima Women White Dial Watch   \n",
       "3  10268  Clarks Men Hang Work Leather Black Formal Shoes   \n",
       "\n",
       "                                         image_bytes color_tags  \n",
       "0  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...       blue  \n",
       "1  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...       blue  \n",
       "2  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01...      white  \n",
       "3  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...      black  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.search().limit(2).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedding Generation\n",
    "\n",
    "Now that we have our text-based features, let's create some vector embeddings. Embeddings are numerical representations of data that capture its semantic meaning. We'll create embeddings for our product images and for our new `summary` and `occasion` features.\n",
    "\n",
    "### Image Embeddings\n",
    "\n",
    "We'll use a pretrained CLIP model to generate embeddings for our product images. We'll define a UDF that takes a batch of image bytes as input, preprocesses them, and then uses the CLIP model to generate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@udf(version=\"0.1\", num_gpus=1 if torch.cuda.is_available() else 0, data_type=pa.list_(pa.float32(), 512))\n",
    "class GenImageEmbeddings(Callable):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.is_loaded=False\n",
    "\n",
    "\n",
    "    def setup(self):\n",
    "        self.model, _, self.preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\")\n",
    "        self.tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = self.model.to(self.device).eval()\n",
    "\n",
    "        self.is_loaded=True\n",
    "\n",
    "    def __call__(self, image_bytes:pa.Array) -> pa.Array:\n",
    "        if not self.is_loaded:\n",
    "            self.setup()\n",
    "\n",
    "        embeddings = []\n",
    "        for b in image_bytes:\n",
    "            this_image_bytes = b.as_buffer().to_pybytes()\n",
    "\n",
    "            image_stream = io.BytesIO(this_image_bytes)\n",
    "            img = Image.open(image_stream).convert(\"RGB\")\n",
    "            img_tensor = self.preprocess(img).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                emb_tensor = self.model.encode_image(img_tensor)\n",
    "                emb_tensor /= emb_tensor.norm(dim=-1, keepdim=True)\n",
    "            np_emb = emb_tensor.squeeze().cpu().numpy().astype(np.float32)\n",
    "\n",
    "            flat = pa.array(np_emb) # 1D float32 vector of shape (512,)\n",
    "            embeddings.append(flat)\n",
    "\n",
    "        stacked = pa.FixedSizeListArray.from_arrays(pa.concat_arrays(embeddings), 512)\n",
    "        return stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(version=\"0.1\", num_gpus=1 if torch.cuda.is_available() else 0, data_type=pa.string())\n",
    "class GenCaptions(Callable):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.is_loaded=False\n",
    "\n",
    "    def setup(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\", use_fast=True)\n",
    "        self.model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        self.model = self.model.to(self.device).eval()\n",
    "        self.is_loaded=True\n",
    "\n",
    "    def __call__(self, image_bytes:pa.Array) -> pa.Array:\n",
    "        if not self.is_loaded:\n",
    "            self.setup()\n",
    "\n",
    "        captions = []\n",
    "        for b in image_bytes:\n",
    "            this_image_bytes = b.as_buffer().to_pybytes()\n",
    "            \n",
    "            image_stream = io.BytesIO(this_image_bytes)\n",
    "            img = Image.open(image_stream).convert(\"RGB\")\n",
    "            \n",
    "            inputs = self.processor(img, return_tensors=\"pt\").to(self.device)\n",
    "            # Use greedy decoding (num_beams=1) and short max_length for speed in this demo\n",
    "            with torch.no_grad():\n",
    "                out = self.model.generate(**inputs, max_length=30, num_beams=3, do_sample=False)\n",
    "            \n",
    "            caption = self.processor.decode(out[0], skip_special_tokens=True)\n",
    "            captions.append(caption)\n",
    "\n",
    "        return pa.array(captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding and Backfilling Embedding and Caption Columns\n",
    "\n",
    "Now, let's add our new generators as virtual columns and then backfill them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:geneva.table:Adding column: udf={'image_embedding': UDF(func=<__main__.GenImageEmbeddings object at 0x4193deea0>, name='GenImageEmbeddings', cuda=False, num_cpus=1.0, num_gpus=0.0, memory=None, batch_size=None, checkpoint_size=None, task_size=None, error_handling=None, input_columns=['image_bytes'], data_type=FixedSizeListType(fixed_size_list<item: float>[512]), version='0.1', _checkpoint_key_override=None, field_metadata={})}\n",
      "/Users/dantasse/src/geneva/src/geneva/table.py:432: UserWarning: Could not validate type for column 'image_bytes' (parameter 'image_bytes') in UDF 'GenImageEmbeddings' with annotation <class 'pyarrow.lib.Array'>. Type validation skipped for this column.\n",
      "  self._validate_udf_input_columns(udf, input_columns)\n",
      "INFO:geneva.table:Adding column: udf={'caption': UDF(func=<__main__.GenCaptions object at 0x3863f53a0>, name='GenCaptions', cuda=False, num_cpus=1.0, num_gpus=0.0, memory=None, batch_size=None, checkpoint_size=None, task_size=None, error_handling=None, input_columns=['image_bytes'], data_type=DataType(string), version='0.1', _checkpoint_key_override=None, field_metadata={})}\n",
      "/Users/dantasse/src/geneva/src/geneva/table.py:432: UserWarning: Could not validate type for column 'image_bytes' (parameter 'image_bytes') in UDF 'GenCaptions' with annotation <class 'pyarrow.lib.Array'>. Type validation skipped for this column.\n",
      "  self._validate_udf_input_columns(udf, input_columns)\n"
     ]
    }
   ],
   "source": [
    "for column in [\"image_embedding\", \"caption\", \"caption_embedding\"]:\n",
    "    if (column in table.schema.names):\n",
    "        table.drop_columns([column])\n",
    "        \n",
    "table.add_columns({\n",
    "    \"image_embedding\": GenImageEmbeddings(),\n",
    "    \"caption\": GenCaptions(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stderr, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dantasse/src/geneva/src/geneva/table.py:816: UserWarning: Could not validate type for column 'image_bytes' (parameter 'image_bytes') in UDF 'GenImageEmbeddings' with annotation <class 'pyarrow.lib.Array'>. Type validation skipped for this column.\n",
      "  validate_backfill_args(self, col_name, udf, read_version=read_version)\n",
      "/Users/dantasse/src/geneva/src/geneva/table.py:704: UserWarning: Could not validate type for column 'image_bytes' (parameter 'image_bytes') in UDF 'GenImageEmbeddings' with annotation <class 'pyarrow.lib.Array'>. Type validation skipped for this column.\n",
      "  validate_backfill_args(self, col_name, udf, read_version=read_version)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f843442e42cf45a390da35e345c1d764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c818f5ed59e45e4bfb21612e21987d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[products - image_embedding (2 fragments)] Rows checkpointed:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fcf4023ecb42e99eb338a3a325ea91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[products - image_embedding (2 fragments)] Rows ready for commit:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2227d54192346deb24cc382b882002d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[products - image_embedding (2 fragments)] Rows committed (every 64 fragments):   0%|          | 0/100 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m \u001b[90m[\u001b[0m2026-01-09T16:29:59Z \u001b[33mWARN \u001b[0m lance::dataset::transaction\u001b[90m]\u001b[0m Building manifest with DataReplacement operation. This operation is not stable yet, please use with caution.\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m Final metric reconciliation timed out; metrics will still be correct on the next tracker flush if the actor stays alive\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'edaaf50f-10f0-4b24-b148-439cdc9fa5a7'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.backfill(\"image_embedding\", checkpoint_size=CHECKPOINT_SIZE, concurrency=CONCURRENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dantasse/src/geneva/src/geneva/table.py:816: UserWarning: Could not validate type for column 'image_bytes' (parameter 'image_bytes') in UDF 'GenCaptions' with annotation <class 'pyarrow.lib.Array'>. Type validation skipped for this column.\n",
      "  validate_backfill_args(self, col_name, udf, read_version=read_version)\n",
      "/Users/dantasse/src/geneva/src/geneva/table.py:704: UserWarning: Could not validate type for column 'image_bytes' (parameter 'image_bytes') in UDF 'GenCaptions' with annotation <class 'pyarrow.lib.Array'>. Type validation skipped for this column.\n",
      "  validate_backfill_args(self, col_name, udf, read_version=read_version)\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m Error running Ray add column operation\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m   File \"/Users/dantasse/src/geneva/src/geneva/runners/ray/pipeline.py\", line 2908, in run_ray_add_column_remote\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m     validate_backfill_args(\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m   File \"/Users/dantasse/src/geneva/src/geneva/runners/ray/pipeline.py\", line 2845, in validate_backfill_args\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m     udf = tbl._conn._packager.unmarshal(udf_spec)\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m   File \"/Users/dantasse/src/geneva/src/geneva/packager/__init__.py\", line 387, in unmarshal\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m     docker_spec = self.backend(spec)\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m                   ^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m   File \"/Users/dantasse/src/geneva/src/geneva/packager/__init__.py\", line 406, in backend\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m     return DockerUDFSpecV1.from_bytes(spec.udf_payload)\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m   File \"/Users/dantasse/src/geneva/src/geneva/packager/__init__.py\", line 172, in from_bytes\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m     val = cls(**self_as_dict)\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m   File \"<attrs generated init geneva.packager.DockerUDFSpecV1>\", line 7, in __init__\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m     self.__attrs_post_init__()\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m   File \"/Users/dantasse/src/geneva/src/geneva/packager/__init__.py\", line 141, in __attrs_post_init__\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m     udf = cloudpickle.loads(self.udf_pickle)\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m   File \"/Users/dantasse/src/vectordb-recipes/venv/lib/python3.12/site-packages/transformers/models/blip/processing_blip.py\", line 22, in <module>\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m     from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n",
      "\u001b[36m(run_ray_add_column_remote pid=83330)\u001b[0m ImportError: cannot import name 'Unpack' from 'transformers.processing_utils' (/Users/dantasse/src/vectordb-recipes/venv/lib/python3.12/site-packages/transformers/processing_utils.py)\n"
     ]
    },
    {
     "ename": "RayTaskError(ImportError)",
     "evalue": "\u001b[36mray::run_ray_add_column_remote()\u001b[39m (pid=83330, ip=127.0.0.1)\n  File \"/Users/dantasse/src/geneva/src/geneva/runners/ray/pipeline.py\", line 2956, in run_ray_add_column_remote\n    raise e\n  File \"/Users/dantasse/src/geneva/src/geneva/runners/ray/pipeline.py\", line 2908, in run_ray_add_column_remote\n    validate_backfill_args(\n  File \"/Users/dantasse/src/geneva/src/geneva/runners/ray/pipeline.py\", line 2845, in validate_backfill_args\n    udf = tbl._conn._packager.unmarshal(udf_spec)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dantasse/src/geneva/src/geneva/packager/__init__.py\", line 387, in unmarshal\n    docker_spec = self.backend(spec)\n                  ^^^^^^^^^^^^^^^^^^\n  File \"/Users/dantasse/src/geneva/src/geneva/packager/__init__.py\", line 406, in backend\n    return DockerUDFSpecV1.from_bytes(spec.udf_payload)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dantasse/src/geneva/src/geneva/packager/__init__.py\", line 172, in from_bytes\n    val = cls(**self_as_dict)\n          ^^^^^^^^^^^^^^^^^^^\n  File \"<attrs generated init geneva.packager.DockerUDFSpecV1>\", line 7, in __init__\n    self.__attrs_post_init__()\n  File \"/Users/dantasse/src/geneva/src/geneva/packager/__init__.py\", line 141, in __attrs_post_init__\n    udf = cloudpickle.loads(self.udf_pickle)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dantasse/src/vectordb-recipes/venv/lib/python3.12/site-packages/transformers/models/blip/processing_blip.py\", line 22, in <module>\n    from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\nImportError: cannot import name 'Unpack' from 'transformers.processing_utils' (/Users/dantasse/src/vectordb-recipes/venv/lib/python3.12/site-packages/transformers/processing_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRayTaskError(ImportError)\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# This may take a few minutes if you're running on a CPU.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackfill\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcaption\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHECKPOINT_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcurrency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONCURRENCY\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/geneva/src/geneva/table.py:846\u001b[39m, in \u001b[36mTable.backfill\u001b[39m\u001b[34m(self, col_name, udf, where, concurrency, intra_applier_concurrency, _admission_check, _admission_strict, refresh_status_secs, _enable_job_tracker_saves, **kwargs)\u001b[39m\n\u001b[32m    843\u001b[39m fut.status()\n\u001b[32m    845\u001b[39m \u001b[38;5;66;03m# Check for errors - this will raise if the job failed\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m846\u001b[39m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[38;5;66;03m# updates came from an external writer, so get the latest version.\u001b[39;00m\n\u001b[32m    849\u001b[39m \u001b[38;5;28mself\u001b[39m._ltbl.checkout_latest()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/geneva/src/geneva/runners/ray/pipeline.py:3206\u001b[39m, in \u001b[36mRayJobFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   3201\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> Any:\n\u001b[32m   3202\u001b[39m     \u001b[38;5;66;03m# TODO this can throw a ray.exceptions.GetTimeoutError if the task\u001b[39;00m\n\u001b[32m   3203\u001b[39m     \u001b[38;5;66;03m# does not complete in time, we should create a new exception type to\u001b[39;00m\n\u001b[32m   3204\u001b[39m     \u001b[38;5;66;03m# encapsulate Ray specifics\u001b[39;00m\n\u001b[32m   3205\u001b[39m     \u001b[38;5;28mself\u001b[39m.status()\n\u001b[32m-> \u001b[39m\u001b[32m3206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mray_obj_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/vectordb-recipes/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py:22\u001b[39m, in \u001b[36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mauto_init_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     21\u001b[39m     auto_init_ray()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/vectordb-recipes/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py:104\u001b[39m, in \u001b[36mclient_mode_hook.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func.\u001b[34m__name__\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33minit\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[32m    103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func.\u001b[34m__name__\u001b[39m)(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/vectordb-recipes/venv/lib/python3.12/site-packages/ray/_private/worker.py:2967\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(object_refs, timeout, _tensor_transport)\u001b[39m\n\u001b[32m   2961\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(object_refs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m   2962\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2963\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid type of object refs, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(object_refs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, is given. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mobject_refs\u001b[39m\u001b[33m'\u001b[39m\u001b[33m must either be an ObjectRef or a list of ObjectRefs. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2965\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2967\u001b[39m values, debugger_breakpoint = \u001b[43mworker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_tensor_transport\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_tensor_transport\u001b[49m\n\u001b[32m   2969\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2970\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n\u001b[32m   2971\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayError):\n\u001b[32m   2972\u001b[39m         \u001b[38;5;66;03m# If the object was lost and it wasn't due to owner death, it may be\u001b[39;00m\n\u001b[32m   2973\u001b[39m         \u001b[38;5;66;03m# because the object store is full and objects needed to be evicted.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/vectordb-recipes/venv/lib/python3.12/site-packages/ray/_private/worker.py:1015\u001b[39m, in \u001b[36mWorker.get_objects\u001b[39m\u001b[34m(self, object_refs, timeout, return_exceptions, skip_deserialization, _tensor_transport)\u001b[39m\n\u001b[32m   1013\u001b[39m     global_worker.core_worker.log_plasma_usage()\n\u001b[32m   1014\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayTaskError):\n\u001b[32m-> \u001b[39m\u001b[32m1015\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value.as_instanceof_cause()\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[31mRayTaskError(ImportError)\u001b[39m: \u001b[36mray::run_ray_add_column_remote()\u001b[39m (pid=83330, ip=127.0.0.1)\n  File \"/Users/dantasse/src/geneva/src/geneva/runners/ray/pipeline.py\", line 2956, in run_ray_add_column_remote\n    raise e\n  File \"/Users/dantasse/src/geneva/src/geneva/runners/ray/pipeline.py\", line 2908, in run_ray_add_column_remote\n    validate_backfill_args(\n  File \"/Users/dantasse/src/geneva/src/geneva/runners/ray/pipeline.py\", line 2845, in validate_backfill_args\n    udf = tbl._conn._packager.unmarshal(udf_spec)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dantasse/src/geneva/src/geneva/packager/__init__.py\", line 387, in unmarshal\n    docker_spec = self.backend(spec)\n                  ^^^^^^^^^^^^^^^^^^\n  File \"/Users/dantasse/src/geneva/src/geneva/packager/__init__.py\", line 406, in backend\n    return DockerUDFSpecV1.from_bytes(spec.udf_payload)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dantasse/src/geneva/src/geneva/packager/__init__.py\", line 172, in from_bytes\n    val = cls(**self_as_dict)\n          ^^^^^^^^^^^^^^^^^^^\n  File \"<attrs generated init geneva.packager.DockerUDFSpecV1>\", line 7, in __init__\n    self.__attrs_post_init__()\n  File \"/Users/dantasse/src/geneva/src/geneva/packager/__init__.py\", line 141, in __attrs_post_init__\n    udf = cloudpickle.loads(self.udf_pickle)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dantasse/src/vectordb-recipes/venv/lib/python3.12/site-packages/transformers/models/blip/processing_blip.py\", line 22, in <module>\n    from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\nImportError: cannot import name 'Unpack' from 'transformers.processing_utils' (/Users/dantasse/src/vectordb-recipes/venv/lib/python3.12/site-packages/transformers/processing_utils.py)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(JobTracker(job_id=edaaf50f-10f0-4b24-b148-439cdc9fa5a7) pid=86132)\u001b[0m error saving metrics lance error: Too many concurrent writers. Attempted 5 times, but failed on retry_timeout of 30.000 seconds., /Users/runner/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/lance-1.0.1/src/dataset/write/retry.rs:55:19\n"
     ]
    }
   ],
   "source": [
    "# This may take a few minutes if you're running on a CPU.\n",
    "table.backfill(\"caption\", checkpoint_size=CHECKPOINT_SIZE, concurrency=CONCURRENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table.search().limit(2).to_pandas().iloc[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features on Features\n",
    "\n",
    "Of course, feature engineering workflows often include chains of features: features that depend on other features we've already computed! Let's make some text embeddings for those captions we just generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(version=\"0.1\", num_gpus=1 if torch.cuda.is_available() else 0, data_type=pa.list_(pa.float32(), 512))\n",
    "class GenTextEmbeddings(Callable):\n",
    "    def __init__(self):\n",
    "        self.is_loaded=False\n",
    "\n",
    "    def setup(self):\n",
    "        self.model, _, self.preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\")\n",
    "        self.tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = self.model.to(self.device).eval()\n",
    "\n",
    "    def __call__(self, caption: pa.Array) -> pa.Array:\n",
    "        if not self.is_loaded:\n",
    "            self.setup()\n",
    "\n",
    "        embeddings = []\n",
    "        for this_caption in caption:\n",
    "            # Convert PyArrow scalar to Python string\n",
    "            caption_str = this_caption.as_py() if hasattr(this_caption, 'as_py') else str(this_caption)\n",
    "            # Tokenizer expects a list of strings, not a single string\n",
    "            tokens = self.tokenizer([caption_str])\n",
    "            tokens = tokens.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                embeddings.append(self.model.encode_text(tokens).squeeze().cpu().numpy().astype(np.float32))\n",
    "\n",
    "        return pa.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"caption_embedding\" in table.schema.names:\n",
    "    table.drop_columns([\"caption_embedding\"])\n",
    "    \n",
    "table.add_columns({\n",
    "    \"caption_embedding\": GenTextEmbeddings(),\n",
    "})\n",
    "table.backfill(\"caption_embedding\", checkpoint_size=CHECKPOINT_SIZE, concurrency=CONCURRENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.search().limit(1).to_pandas().iloc[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Updating\n",
    "\n",
    "Let's add some new clothes to our table and rerun the backfills to add all our derived features. This will only recompute our backfills on the new rows. This doesn't save that much time in this tutorial, but it absolutely does in production. Imagine adding new data daily; you won't want to recompute your costly features on all your data every day!\n",
    "\n",
    "As we do the following backfills, you will notice this as the progress bars start at \"1000/1500\", reflecting that the original 1000 rows have already been computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rows = list(generate_rows(df, IMG_DIR, start=DATASET_SIZE, end=DATASET_SIZE+500))\n",
    "table.add(new_rows)\n",
    "table.backfill(\"color_tags\", checkpoint_size=CHECKPOINT_SIZE, concurrency=CONCURRENCY)\n",
    "table.backfill(\"image_embedding\", checkpoint_size=CHECKPOINT_SIZE, concurrency=CONCURRENCY)\n",
    "table.backfill(\"caption\", checkpoint_size=CHECKPOINT_SIZE, concurrency=CONCURRENCY)\n",
    "table.backfill(\"caption_embedding\", checkpoint_size=CHECKPOINT_SIZE, concurrency=CONCURRENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Wrapping up\n",
    "\n",
    "That's the basics of feature engineering! If you wanted to go on to query this table, you might build indexes, as in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Full-Text Search (FTS) Index: This will allow us to quickly search for keywords in our `caption` column.\n",
    "table.create_fts_index(\"caption\")\n",
    "# Vector Index: This will allow us to perform fast similarity searches on our `image_embedding` column.\n",
    "table.create_index(vector_column_name=\"image_embedding\", num_sub_vectors=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you want to expand this to run on a Ray cluster, to scale up to production use, see [Execution Contexts](https://docs.lancedb.com/geneva/jobs/contexts) for more info on how to do so."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering with LanceDB and Geneva\n",
    "\n",
    "This notebook will focus on the crucial process of feature engineering. We'll start with a raw dataset of fashion products, ingest it in LanceDB, then enrich our data with meaningful features that we could use to build a search engine or train a model.\n",
    "\n",
    "We will cover the following steps:\n",
    "1. **Data Ingestion**: Downloading a fashion dataset and loading it into a LanceDB table.\n",
    "2. **Declarative Feature Engineering**: Using Geneva to define and compute features on-the-fly.\n",
    "3. **Embedding Generation**: Creating vector embeddings for both images and text to enable semantic search.\n",
    "4. **Updating**: Adding more raw data to our table and rerunning our backfills on only the new data.\n",
    "\n",
    "## Note about Colab\n",
    "\n",
    "This notebook runs on Google Colab, even the free tier, but it will be slow, because it has to start a local Ray cluster and execute multiple ML models on its workers. We recommend downloading this notebook and running it locally. But if you do run on Colab, we recommend:\n",
    "- using a GPU instance (Runtime -> Change runtime type)\n",
    "- running on only 100 rows\n",
    "- not drawing conclusions about speed from this notebook. This notebook is meant as a demo of the basic workflow of feature engineering with LanceDB, not a benchmark or speed demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade geneva lancedb pylance\n",
    "!pip install kubernetes \"ray[default]\" rerankers pandas torch torchvision open-clip-torch transformers\n",
    "\n",
    "# Pin protobuf to avoid MessageFactory.GetPrototype AttributeError warnings (removed in protobuf 6.30.0+)\n",
    "!pip install \"protobuf<6.30.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion\n",
    "\n",
    "First, let's download our dataset. We're using a small version of the Fashion Product Images dataset from Kaggle. This dataset contains images and metadata for a variety of fashion products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!sudo rm -r db fashion-dataset # Uncomment and run this to delete the dataset if it already exists\n",
    "\n",
    "# Download the dataset if it doesn't exist\n",
    "!test -d fashion-dataset && test -n \"$(ls -A fashion-dataset 2>/dev/null)\" && \\\n",
    "  echo \"Dataset already exists, skipping download\" || \\\n",
    "    (curl -L -o fashion-product-images-small.zip https://www.kaggle.com/api/v1/datasets/download/paramaggarwal/fashion-product-images-small \\\n",
    "    && unzip -q fashion-product-images-small.zip -d fashion-dataset/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Scale based on your environment\n",
    "\n",
    "This tutorial uses Ray locally to build features, which means the scale of concurrent jobs will be limited to the system you're working on. These parameters are good defaults, but feel free to adjust them if you'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Especially if on Colab, start with just 100 rows for testing.\n",
    "DATASET_SIZE = 100\n",
    "# Increase this if you're running locally, you have more CPUs available and want it to run faster.\n",
    "CONCURRENCY = 4\n",
    "CHECKPOINT_SIZE = min(300, DATASET_SIZE / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import geneva\n",
    "from geneva import udf\n",
    "import lancedb\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "import open_clip\n",
    "from typing import Callable\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "IMG_DIR = Path(\"fashion-dataset/images\")\n",
    "STYLE_CSV = Path(\"fashion-dataset/styles.csv\")\n",
    "DB_PATH = \"./db\"\n",
    "TABLE_NAME = \"products\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the data into a LanceDB table. We'll read the CSV file with the product metadata, and for each product, we'll also load the corresponding image from the `images` directory. We'll then create a LanceDB table and add the data to it in batches. LanceDB can store objects(images in this case) along with vector embeddings and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STYLE_CSV has info about the clothes: IDs, descriptions, and image paths\n",
    "# Images themselves are stored in IMG_DIR; generate_rows will combine them with the metadata\n",
    "# in STYLE_CSV and load them all in to our LanceDB table.\n",
    "df = pd.read_csv(STYLE_CSV, on_bad_lines='skip')\n",
    "df = df.dropna(subset=[\"id\", \"productDisplayName\"])\n",
    "df = df.drop_duplicates(subset=[\"id\"], keep=\"first\")\n",
    "\n",
    "def generate_rows(df, img_dir, start=0, end=DATASET_SIZE):\n",
    "    for _, row in df.iloc[start:end].iterrows():\n",
    "        img_path = img_dir / f\"{row['id']}.jpg\"\n",
    "        if not img_path.exists():\n",
    "            continue\n",
    "        with open(img_path, \"rb\") as f:\n",
    "            yield {\n",
    "                \"id\": int(row[\"id\"]),\n",
    "                \"description\": row[\"productDisplayName\"],\n",
    "                \"image_bytes\": f.read()\n",
    "            }\n",
    "\n",
    "\n",
    "db = lancedb.connect(DB_PATH)\n",
    "# Drop the table if it already exists so we can recreate it\n",
    "try:\n",
    "    table = db.drop_table(TABLE_NAME)\n",
    "except ValueError as e:\n",
    "    pass\n",
    "    \n",
    "data_stream = generate_rows(df, IMG_DIR)\n",
    "table = None\n",
    "\n",
    "# Create the table and load rows in in batches\n",
    "rows = []\n",
    "for row in data_stream:\n",
    "    rows.append(row)\n",
    "    if len(rows) == min(1000, DATASET_SIZE / 2):\n",
    "        if table:\n",
    "            table.add(rows)\n",
    "        else:\n",
    "            # You have to provide schema or some data to create the table. Here we create the \n",
    "            # table with the first batch of data for simplicity.\n",
    "            table = db.create_table(TABLE_NAME, data=rows)\n",
    "        rows = []\n",
    "if rows:\n",
    "    table.add(rows)\n",
    "    \n",
    "len(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering with Geneva\n",
    "\n",
    "Now that we have our data in a LanceDB table, we can start engineering features. We'll use Geneva to create new features for our products. \n",
    "\n",
    "### Defining geneva UDF\n",
    "\n",
    "Geneva uses Python User Defined Functions (UDFs) to define features as columns in a Lance dataset. Adding a feature is straightforward:\n",
    "\n",
    "1. Prototype your Python function in your favorite environment.\n",
    "2. Wrap the function with small UDF decorator.\n",
    "3. Register the UDF as a virtual column using Table.add_columns().\n",
    "4. Trigger a backfill operation\n",
    "\n",
    "UDFs can work on one row or a batch at a time, and can be stateful (e.g. some work is done to set up a model the first time, and future runs use the same model) or stateless. [Read more about geneva UDFs here.](https://docs.lancedb.com/geneva/udfs)\n",
    "\n",
    "### Simple Feature Extraction\n",
    "\n",
    "Let's start with a simple feature: extracting color tags from the product description. We'll define a User-Defined Function (UDF) that takes the product description as input and returns a comma-separated string of colors found in the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = geneva.connect(DB_PATH)\n",
    "table = db.open_table(TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@udf\n",
    "def color_tags(description: str)-> str:\n",
    "    colors = [\"black\", \"white\", \"red\", \"blue\", \"green\", \"yellow\", \"pink\", \"brown\", \"grey\", \"silver\"]\n",
    "    return \", \".join([c for c in colors if c in description.lower()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Computed Column\n",
    "\n",
    "Now that we've defined our feature-generating UDF, we can add it to our table as a computed column. Computed columns are computed on-the-fly when you perform a backfill operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if \"color_tags\" in table.schema.names:\n",
    "    table.drop_columns([\"color_tags\"])\n",
    "    \n",
    "table.add_columns({\n",
    "    \"color_tags\": color_tags,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the table schema to see our newly registered UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backfilling Features\n",
    "\n",
    "Triggering backfill creates a distributed job to run the UDF and populate the column values in your LanceDB table. The Geneva framework simplifies several aspects of distributed execution.\n",
    "\n",
    "Checkpoints: Each batch of UDF execution is checkpointed so that partial results are not lost in case of job failures. Jobs can resume and avoid most of the expense of having to recalculate values.\n",
    "\n",
    "`backfill()` accepts various params to customize scale of your workload, here we'll use:\n",
    "\n",
    "* `checkpoint_size` - the number of rows that are processed before writing a checkpoint\n",
    "* `concurrency` - how many nodes are used for parallelization\n",
    "\n",
    "Here, we'll use `db.local_ray_context()` to run on a local Ray instance, so we don't need to set up a Ray cluster, but you can also use the same setup and run distributed jobs remotely on Ray clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with db.local_ray_context():\n",
    "    table.backfill(\"color_tags\", checkpoint_size=CHECKPOINT_SIZE, concurrency=CONCURRENCY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our enriched data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table.search().limit(5).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedding Generation\n",
    "\n",
    "Now that we have our text-based features, let's create some vector embeddings. Embeddings are numerical representations of data that capture its semantic meaning. We'll create embeddings for our product images and for our new `summary` and `occasion` features.\n",
    "\n",
    "### Image Embeddings\n",
    "\n",
    "We'll use a pretrained CLIP model to generate embeddings for our product images. We'll define a UDF that takes a batch of image bytes as input, preprocesses them, and then uses the CLIP model to generate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@udf(version=\"0.1\", num_gpus=1 if torch.cuda.is_available() else 0, data_type=pa.list_(pa.float32(), 512))\n",
    "class GenImageEmbeddings(Callable):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.is_loaded=False\n",
    "\n",
    "\n",
    "    def setup(self):\n",
    "        self.model, _, self.preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\")\n",
    "        self.tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = self.model.to(self.device).eval()\n",
    "\n",
    "        self.is_loaded=True\n",
    "\n",
    "    def __call__(self, image_bytes:pa.Array) -> pa.Array:\n",
    "        if not self.is_loaded:\n",
    "            self.setup()\n",
    "\n",
    "        embeddings = []\n",
    "        for b in image_bytes:\n",
    "            this_image_bytes = b.as_buffer().to_pybytes()\n",
    "\n",
    "            image_stream = io.BytesIO(this_image_bytes)\n",
    "            img = Image.open(image_stream).convert(\"RGB\")\n",
    "            img_tensor = self.preprocess(img).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                emb_tensor = self.model.encode_image(img_tensor)\n",
    "                emb_tensor /= emb_tensor.norm(dim=-1, keepdim=True)\n",
    "            np_emb = emb_tensor.squeeze().cpu().numpy().astype(np.float32)\n",
    "\n",
    "            flat = pa.array(np_emb) # 1D float32 vector of shape (512,)\n",
    "            embeddings.append(flat)\n",
    "\n",
    "        stacked = pa.FixedSizeListArray.from_arrays(pa.concat_arrays(embeddings), 512)\n",
    "        return stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(version=\"0.1\", num_gpus=1 if torch.cuda.is_available() else 0, data_type=pa.string())\n",
    "class GenCaptions(Callable):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.is_loaded=False\n",
    "\n",
    "    def setup(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\", use_fast=True)\n",
    "        self.model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        self.model = self.model.to(self.device).eval()\n",
    "        self.is_loaded=True\n",
    "\n",
    "    def __call__(self, image_bytes:pa.Array) -> pa.Array:\n",
    "        if not self.is_loaded:\n",
    "            self.setup()\n",
    "\n",
    "        captions = []\n",
    "        for b in image_bytes:\n",
    "            this_image_bytes = b.as_buffer().to_pybytes()\n",
    "            \n",
    "            image_stream = io.BytesIO(this_image_bytes)\n",
    "            img = Image.open(image_stream).convert(\"RGB\")\n",
    "            \n",
    "            inputs = self.processor(img, return_tensors=\"pt\").to(self.device)\n",
    "            # Use greedy decoding (num_beams=1) and short max_length for speed in this demo\n",
    "            with torch.no_grad():\n",
    "                out = self.model.generate(**inputs, max_length=30, num_beams=3, do_sample=False)\n",
    "            \n",
    "            caption = self.processor.decode(out[0], skip_special_tokens=True)\n",
    "            captions.append(caption)\n",
    "\n",
    "        return pa.array(captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding and Backfilling Embedding and Caption Columns\n",
    "\n",
    "Now, let's add our new generators as virtual columns and then backfill them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for column in [\"image_embedding\", \"caption\", \"caption_embedding\"]:\n",
    "    if (column in table.schema.names):\n",
    "        table.drop_columns([column])\n",
    "        \n",
    "table.add_columns({\n",
    "    \"image_embedding\": GenImageEmbeddings(),\n",
    "    \"caption\": GenCaptions(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stderr, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with db.local_ray_context():\n",
    "    table.backfill(\"image_embedding\", checkpoint_size=CHECKPOINT_SIZE, concurrency=CONCURRENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This may take a few minutes if you're running on a CPU.\n",
    "with db.local_ray_context():\n",
    "    table.backfill(\"caption\", checkpoint_size=CHECKPOINT_SIZE, concurrency=CONCURRENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table.search().limit(5).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features on Features\n",
    "\n",
    "Of course, feature engineering workflows often include chains of features: features that depend on other features we've already computed! Let's make some text embeddings for those captions we just generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(version=\"0.1\", num_gpus=1 if torch.cuda.is_available() else 0, data_type=pa.list_(pa.float32(), 512))\n",
    "class GenTextEmbeddings(Callable):\n",
    "    def __init__(self):\n",
    "        self.is_loaded=False\n",
    "\n",
    "    def setup(self):\n",
    "        self.model, _, self.preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\")\n",
    "        self.tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = self.model.to(self.device).eval()\n",
    "\n",
    "    def __call__(self, caption: pa.Array) -> pa.Array:\n",
    "        if not self.is_loaded:\n",
    "            self.setup()\n",
    "\n",
    "        embeddings = []\n",
    "        for this_caption in caption:\n",
    "            # Convert PyArrow scalar to Python string\n",
    "            caption_str = this_caption.as_py() if hasattr(this_caption, 'as_py') else str(this_caption)\n",
    "            # Tokenizer expects a list of strings, not a single string\n",
    "            tokens = self.tokenizer([caption_str])\n",
    "            tokens = tokens.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                embeddings.append(self.model.encode_text(tokens).squeeze().cpu().numpy().astype(np.float32))\n",
    "\n",
    "        return pa.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"caption_embedding\" in table.schema.names:\n",
    "    table.drop_columns([\"caption_embedding\"])\n",
    "    \n",
    "table.add_columns({\n",
    "    \"caption_embedding\": GenTextEmbeddings(),\n",
    "})\n",
    "with db.local_ray_context():\n",
    "    table.backfill(\"caption_embedding\", checkpoint_size=CHECKPOINT_SIZE, concurrency=CONCURRENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.search().limit(5).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Updating\n",
    "\n",
    "Let's add some new clothes to our table and rerun the backfills to add all our derived features. This will only recompute our backfills on the new rows. This doesn't save that much time in this tutorial, but it absolutely does in production. Imagine adding new data daily; you won't want to recompute your costly features on all your data every day!\n",
    "\n",
    "We'll add another batch of data that's half as big as the first. Then, as we do the following backfills, you will notice this as the progress bars only process the new data. For example, if our dataset was originally 100 rows, and we add 50 more, progress bars will show \"100/150\", reflecting that the original 100 rows have already been computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rows = list(generate_rows(df, IMG_DIR, start=DATASET_SIZE, end=int(DATASET_SIZE * 1.5)))\n",
    "table.add(new_rows)\n",
    "with db.local_ray_context():\n",
    "    table.backfill(\"color_tags\", checkpoint_size=CHECKPOINT_SIZE, concurrency=CONCURRENCY)\n",
    "    table.backfill(\"image_embedding\", checkpoint_size=CHECKPOINT_SIZE, concurrency=CONCURRENCY)\n",
    "    table.backfill(\"caption\", checkpoint_size=CHECKPOINT_SIZE, concurrency=CONCURRENCY)\n",
    "    table.backfill(\"caption_embedding\", checkpoint_size=CHECKPOINT_SIZE, concurrency=CONCURRENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Wrapping up\n",
    "\n",
    "That's the basics of feature engineering! Next, check out our other [feature engineering tutorials](https://docs.lancedb.com/tutorials/feature-engineering), or if you're ready to start building features in production, read about [Execution Contexts](https://docs.lancedb.com/geneva/jobs/contexts)."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

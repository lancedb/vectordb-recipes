{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial is divided into two parts:\n",
    "\n",
    "*   **Part 1: Feature Engineering with LanceDB and Geneva**: In this part, we'll focus on the crucial process of feature engineering. We'll use LanceDB and its Geneva feature engineering framework to enrich our data with meaningful features that will power our search engine.\n",
    "\n",
    "*   **Part 2: Inference and Retrieval with LanceDB**: In this part, we'll build the inference and retrieval pipeline that uses the features we engineered in Part 1 to provide a powerful and intuitive search experience. We'll cover query routing, hybrid search, and reranking to build a state-of-the-art search engine. \n",
    "\n",
    "## Part 1: Feature Engineering with LanceDB and Geneva\n",
    "\n",
    "This notebook is the first part of our tutorial on building an advanced product search engine. In this part, we will focus on the crucial process of feature engineering. We'll start with a raw dataset of fashion products, and ingest it in LanceDB. We'll then use Geneva to enrich our data with meaningful features that will power our search engine.\n",
    "\n",
    "We will cover the following steps:\n",
    "1. **Data Ingestion**: Downloading a fashion dataset and loading it into a LanceDB table.\n",
    "2. **Declarative Feature Engineering**: Using Geneva to define and compute features on-the-fly.\n",
    "3. **Embedding Generation**: Creating vector embeddings for both images and text to enable semantic search.\n",
    "4. **Indexing**: Creating indexes to speed up our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade geneva lancedb kubernetes \"ray[default]\" rerankers pandas pillow transformers torch torchvision open-clip-torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion\n",
    "\n",
    "First, let's download our dataset. We're using a small version of the Fashion Product Images dataset from Kaggle. This dataset contains images and metadata for a variety of fashion products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sudo rm -r db fashion-dataset # Delete if already exists\n",
    "\n",
    "# ### HIGH RES DATASET (Slow to download) #\n",
    "#!curl -L -o fashion-product-images-dataset.zip\\\n",
    "#  https://www.kaggle.com/api/v1/datasets/download/paramaggarwal/fashion-product-images-dataset\n",
    "\n",
    "#!unzip -q fashion-product-images-dataset.zip \n",
    "\n",
    "# SAME DATASET WITH LOW RES IMAGES #\n",
    "\n",
    "!curl -L -o fashion-product-images-small.zip\\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/paramaggarwal/fashion-product-images-small\n",
    "!unzip -q fashion-product-images-small.zip -d fashion-dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Scale based on your environment\n",
    "\n",
    "This example uses geneva locally by default - which means the scale of concurrent jobs will be limited to the system you're working on. Set these params based on your CPU/GPU and memory configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Update this based on your env\n",
    "\n",
    "# Start with 100-1000 images for testing. With only a few images, you might not get great query\n",
    "# results, but it will run faster. Try up to 44200 for the full dataset.\n",
    "DATASET_SIZE = 1000 \n",
    "# Increase this if you have more CPUs available and want it to run faster.\n",
    "CONCURRENCY = 4\n",
    "\n",
    "CHECKPOINT_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import geneva\n",
    "from geneva import udf\n",
    "import lancedb\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "import open_clip\n",
    "from typing import Callable\n",
    "\n",
    "IMG_DIR = Path(\"fashion-dataset/images\")\n",
    "STYLE_CSV = Path(\"fashion-dataset/styles.csv\")\n",
    "DB_PATH = \"./db\"\n",
    "TABLE_NAME = \"products\"\n",
    "INSERT_FRAG_SIZE = min(1000, DATASET_SIZE / 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the data into a LanceDB table. We'll read the CSV file with the product metadata, and for each product, we'll also load the corresponding image from the `images` directory. We'll then create a LanceDB table and add the data to it in batches. LanceDB can store objects(images in this case) along with vector embeddings and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(STYLE_CSV, on_bad_lines='skip')\n",
    "df = df.dropna(subset=[\"id\", \"productDisplayName\"])    \n",
    "df = df.drop_duplicates(subset=[\"id\"], keep=\"first\")    \n",
    "df = df.sample(DATASET_SIZE) # set to 1000 for testing\n",
    "\n",
    "def generate_rows(df, img_dir):\n",
    "    for _, row in df.iterrows():\n",
    "        img_path = img_dir / f\"{row['id']}.jpg\"\n",
    "        if not img_path.exists():\n",
    "            continue\n",
    "        with open(img_path, \"rb\") as f:\n",
    "            yield {\n",
    "                \"id\": int(row[\"id\"]),\n",
    "                \"description\": row[\"productDisplayName\"],\n",
    "                \"image_bytes\": f.read()\n",
    "            }\n",
    "\n",
    "db = lancedb.connect(DB_PATH)\n",
    "\n",
    "# Drop the table if it already exists so we can recreate it\n",
    "try:\n",
    "    table = db.drop_table(TABLE_NAME)\n",
    "except ValueError as e:\n",
    "    pass\n",
    "    \n",
    "data_stream = generate_rows(df, IMG_DIR)\n",
    "table = None\n",
    "\n",
    "rows = []\n",
    "for row in data_stream:\n",
    "    rows.append(row)\n",
    "    if len(rows) == INSERT_FRAG_SIZE:\n",
    "        if table:\n",
    "            table.add(rows)\n",
    "        else:\n",
    "            table = db.create_table(TABLE_NAME, data=rows)\n",
    "        rows = []\n",
    "if rows:\n",
    "    table.add(rows)\n",
    "    \n",
    "len(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering with Geneva\n",
    "\n",
    "Now that we have our data in a LanceDB table, we can start engineering features. We'll use Geneva to create new features for our products. \n",
    "\n",
    "### Defining geneva UDF\n",
    "\n",
    "Geneva uses Python User Defined Functions (UDFs) to define features as columns in a Lance dataset. Adding a feature is straightforward:\n",
    "\n",
    "1. Prototype your Python function in your favorite environment.\n",
    "2. Wrap the function with small UDF decorator.\n",
    "3. Register the UDF as a virtual column using Table.add_columns().\n",
    "4. Trigger a backfill operation.\n",
    "There are various kinds of UDFs you can use depending on the task type\n",
    "\n",
    "* **Row-level, stateless UDFs** - You can use these when you're tasks don't need to be optimized with batch processing, and they don't require complex setup each time\n",
    "* **Row-level, stateful UDFs** - You can use these when you're tasks don't need to be optimized with batch processing, and they require complex setup each time\n",
    "* **Batched, Statless UDFs** - You can use these when batch processing is faster but you don't require complex setup each time.\n",
    "* **Batched, Stateful UDFs** - You can use these when batch processing is faster AND you require complex setup (like loading model) for each batch.\n",
    "Read more about geneva UDFs here - TODO: Add new docs link\n",
    "\n",
    "In this example we'll use Batched, Stateful UDF\n",
    "\n",
    "NOTE: num_gpus>0 means this UDF is meant to run on GPU nodes\n",
    "\n",
    "### Simple Feature Extraction\n",
    "\n",
    "Let's start with a simple feature: extracting color tags from the product description. We'll define a User-Defined Function (UDF) that takes the product description as input and returns a comma-separated string of colors found in the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = geneva.connect(DB_PATH)\n",
    "table = db.open_table(TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@udf\n",
    "def color_tags(description: str)-> str:\n",
    "    colors = [\"black\", \"white\", \"red\", \"blue\", \"green\", \"yellow\", \"pink\", \"brown\", \"grey\", \"silver\"]\n",
    "    return \", \".join([c for c in colors if c in description.lower()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Computed Column\n",
    "\n",
    "Now that we've defined our feature-generating UDF, we can add it to our table as a computed column. Computed columns are computed on-the-fly when you perform a backfill operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table.add_columns({\n",
    "    \"color_tags\": color_tags,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the table schema to see our newly registered UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backfilling Features\n",
    "\n",
    "Triggering backfill creates a distributed job to run the UDF and populate the column values in your LanceDB table. The Geneva framework simplifies several aspects of distributed execution.\n",
    "\n",
    "Environment management: Geneva automatically packages and deploys your Python execution environment to worker nodes. This ensures that distributed execution occurs in the same environment and depedencies as your prototype.\n",
    "\n",
    "Checkpoints: Each batch of UDF execution is checkpointed so that partial results are not lost in case of job failures. Jobs can resume and avoid most of the expense of having to recalculate values.\n",
    "\n",
    "backfill accepts various params to customise scale of your workload, here we'll use:\n",
    "\n",
    "* **checkpoint_size**  - Which determines the number of rows that are processed before writing a checkpoint\n",
    "* **concurrency** - Which determins how many nodes are used for parallelization\n",
    "\n",
    "Here, we're using geneva locally, so we won't set up a Ray cluster, but you can also use the same setup and run distributed jobs remotely on Ray clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "table.backfill(\"color_tags\", checkpoint_size=CHECKPOINT_SIZE, concurrency=CONCURRENCY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our enriched data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table.search().limit(3).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedding Generation\n",
    "\n",
    "Now that we have our text-based features, let's create some vector embeddings. Embeddings are numerical representations of data that capture its semantic meaning. We'll create embeddings for our product images and for our new `summary` and `occasion` features.\n",
    "\n",
    "### Image Embeddings\n",
    "\n",
    "We'll use a pretrained CLIP model to generate embeddings for our product images. We'll define a UDF that takes a batch of image bytes as input, preprocesses them, and then uses the CLIP model to generate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "@udf(version=\"0.1\", num_gpus=1 if torch.cuda.is_available() else 0, data_type=pa.list_(pa.float32(), 512))\n",
    "class GenEmbeddings(Callable):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.is_loaded=False\n",
    "\n",
    "\n",
    "    def setup(self):\n",
    "        self.model, _, self.preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\")\n",
    "        self.tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = self.model.to(self.device).eval()\n",
    "\n",
    "        self.is_loaded=True\n",
    "\n",
    "    def __call__(self, image_bytes:pa.Array) -> pa.Array:\n",
    "        if not self.is_loaded:\n",
    "            self.setup()\n",
    "\n",
    "        embeddings = []\n",
    "        for b in image_bytes:\n",
    "            this_image_bytes = b.as_buffer().to_pybytes()\n",
    "\n",
    "            image_stream = io.BytesIO(this_image_bytes)\n",
    "            img = Image.open(image_stream).convert(\"RGB\")\n",
    "            img_tensor = self.preprocess(img).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                emb_tensor = self.model.encode_image(img_tensor)\n",
    "                emb_tensor /= emb_tensor.norm(dim=-1, keepdim=True)\n",
    "            np_emb = emb_tensor.squeeze().cpu().numpy().astype(np.float32)\n",
    "\n",
    "            flat = pa.array(np_emb) # 1D float32 vector of shape (512,)\n",
    "            embeddings.append(flat)\n",
    "\n",
    "        stacked = pa.FixedSizeListArray.from_arrays(pa.concat_arrays(embeddings), 512)\n",
    "        return stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "@udf(version=\"0.1\", num_gpus=1 if torch.cuda.is_available() else 0, data_type=pa.string())\n",
    "class GenCaptions(Callable):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.is_loaded=False\n",
    "\n",
    "    def setup(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        self.model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        self.model = self.model.to(self.device).eval()\n",
    "        self.is_loaded=True\n",
    "\n",
    "    def __call__(self, image_bytes:pa.Array) -> pa.Array:\n",
    "        if not self.is_loaded:\n",
    "            self.setup()\n",
    "\n",
    "        captions = []\n",
    "        for b in image_bytes:\n",
    "            this_image_bytes = b.as_buffer().to_pybytes()\n",
    "            \n",
    "            image_stream = io.BytesIO(this_image_bytes)\n",
    "            img = Image.open(image_stream).convert(\"RGB\")\n",
    "            \n",
    "            inputs = self.processor(img, return_tensors=\"pt\").to(self.device)\n",
    "            # Use greedy decoding (num_beams=1) and short max_length for speed in this demo\n",
    "            with torch.no_grad():\n",
    "                out = self.model.generate(**inputs, max_length=30, num_beams=1, do_sample=False)\n",
    "            \n",
    "            caption = self.processor.decode(out[0], skip_special_tokens=True)\n",
    "            captions.append(caption)\n",
    "\n",
    "        return pa.array(captions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding and Backfilling Embedding Columns\n",
    "\n",
    "Now, let's add our new embedding generators as virtual columns and then backfill them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for column in [\"image_embedding\", \"captions\"]:\n",
    "    if (column in table.schema.names):\n",
    "        table.drop_columns([column])\n",
    "        \n",
    "table.add_columns({\n",
    "    \"image_embedding\": GenEmbeddings(),\n",
    "    \"caption\": GenCaptions(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stderr, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.backfill(\"image_embedding\", checkpoint_size=CHECKPOINT_SIZE, concurrency=CONCURRENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This may take a few minutes if you're running on a CPU.\n",
    "table.backfill(\"caption\", checkpoint_size=CHECKPOINT_SIZE, concurrency=CONCURRENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table.search().limit(20).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Indexing\n",
    "\n",
    "To speed up our queries, we need to create indexes on our new features. We'll create two types of indexes:\n",
    "\n",
    "*   **Full-Text Search (FTS) Index**: This will allow us to quickly search for keywords in our `description` column.\n",
    "*   **Vector Index**: This will allow us to perform fast similarity searches on our `image_embedding` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table.create_fts_index(\"caption\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table.create_index(vector_column_name=\"image_embedding\", num_sub_vectors=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table.list_indices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for the feature engineering part! We now have a LanceDB table enriched with a variety of features that will power our search engine. In the next part of this tutorial, we'll build the inference and retrieval pipeline that uses these features to provide a powerful and intuitive search experience."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serverless QA Bot with Modal and LangChain\n",
    "\n",
    "## use LanceDB's LangChain integration with Modal to run a serverless app\n",
    "\n",
    "<img id=\"splash\" width=\"400\" alt=\"modal\" src=\"https://github.com/lancedb/lancedb/assets/917119/7d80a40f-60d7-48a6-972f-dab05000eccf\">\n",
    "\n",
    "We're going to build a QA bot for your documentation using LanceDB's LangChain integration and use Modal for deployment.\n",
    "\n",
    "Modal is an end-to-end compute platform for model inference, batch jobs, task queues, web apps and more. It's a great way to deploy your LanceDB models and apps.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To get started, ensure that you have created an account and logged into [Modal](https://modal.com/). To follow along, the full source code is available on Github [here](https://github.com/lancedb/lancedb/blob/main/docs/src/examples/modal_langchain.py)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Modal\n",
    "\n",
    "We'll start by specifying our dependencies and creating a new Modal `Stub`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lancedb_image = Image.debian_slim().pip_install(\n",
    "    \"lancedb\",\n",
    "    \"langchain\",\n",
    "    \"openai\",\n",
    "    \"pandas\",\n",
    "    \"tiktoken\",\n",
    "    \"unstructured\",\n",
    "    \"tabulate\"\n",
    ")\n",
    "\n",
    "stub = Stub(\n",
    "    name=\"example-langchain-lancedb\",\n",
    "    image=lancedb_image,\n",
    "    secrets=[Secret.from_name(\"my-openai-secret\")],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using Modal's Secrets injection to secure our OpenAI key. To set your own, you can access the Modal UI and enter your key."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up caches for LanceDB and LangChain\n",
    "\n",
    "Next, we can setup some globals to cache our LanceDB database, as well as our LangChain docsource:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = None\n",
    "docs_path = Path(\"docs.pkl\")\n",
    "db_path = Path(\"lancedb\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading our dataset\n",
    "\n",
    "We're going use a pregenerated dataset, which stores HTML files of the Pandas 2.0 documentation. \n",
    "You could switch this out for your own dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_docs():\n",
    "    pandas_docs = requests.get(\"https://eto-public.s3.us-west-2.amazonaws.com/datasets/pandas_docs/pandas.documentation.zip\")\n",
    "    with open(Path(\"pandas.documentation.zip\"), \"wb\") as f:\n",
    "        f.write(pandas_docs.content)\n",
    "\n",
    "    file = zipfile.ZipFile(Path(\"pandas.documentation.zip\"))\n",
    "    file.extractall(path=Path(\"pandas_docs\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the dataset and generating metadata\n",
    "\n",
    "Once we've downloaded it, we want to parse and pre-process them using LangChain, and then vectorize them and store it in LanceDB.\n",
    "Let's first create a function that uses LangChains `UnstructuredHTMLLoader` to parse them.\n",
    "We can then add our own metadata to it and store it alongside the data, we'll later be able to use this for filtering metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_docs():\n",
    "    docs = []\n",
    "\n",
    "    if not docs_path.exists():\n",
    "        for p in Path(\"pandas_docs/pandas.documentation\").rglob(\"*.html\"):\n",
    "            if p.is_dir():\n",
    "                continue\n",
    "            loader = UnstructuredHTMLLoader(p)\n",
    "            raw_document = loader.load()\n",
    "\n",
    "            m = {}\n",
    "            m[\"title\"] = get_document_title(raw_document[0])\n",
    "            m[\"version\"] = \"2.0rc0\"\n",
    "            raw_document[0].metadata = raw_document[0].metadata | m\n",
    "            raw_document[0].metadata[\"source\"] = str(raw_document[0].metadata[\"source\"])\n",
    "            docs = docs + raw_document\n",
    "\n",
    "        with docs_path.open(\"wb\") as fh:\n",
    "            pickle.dump(docs, fh)\n",
    "    else:\n",
    "        with docs_path.open(\"rb\") as fh:\n",
    "            docs = pickle.load(fh)\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple LangChain chain for a QA bot\n",
    "\n",
    "Now we can create a simple LangChain chain for our QA bot. We'll use the `RecursiveCharacterTextSplitter` to split our documents into chunks, and then use the `OpenAIEmbeddings` to vectorize them.\n",
    "\n",
    "Lastly, we'll create a LanceDB table and store the vectorized documents in it, then create a `RetrievalQA` model from the chain and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qanda_langchain(query):\n",
    "    download_docs()\n",
    "    docs = store_docs()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "    )\n",
    "    documents = text_splitter.split_documents(docs)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    db = lancedb.connect(db_path) \n",
    "    table = db.create_table(\"pandas_docs\", data=[\n",
    "        {\"vector\": embeddings.embed_query(\"Hello World\"), \"text\": \"Hello World\", \"id\": \"1\"}\n",
    "    ], mode=\"overwrite\")\n",
    "    docsearch = LanceDB.from_documents(documents, embeddings, connection=table)\n",
    "    qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever())\n",
    "    return qa.run(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our Modal entry points\n",
    "\n",
    "Now we can create our Modal entry points for our CLI and web endpoint:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@stub.function()\n",
    "@web_endpoint(method=\"GET\")\n",
    "def web(query: str):\n",
    "    answer = qanda_langchain(query)\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "    }\n",
    "    \n",
    "@stub.function()\n",
    "def cli(query: str):\n",
    "    answer = qanda_langchain(query)\n",
    "    print(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing it out!\n",
    "\n",
    "Testing the CLI:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%bash\n",
    "modal run modal_langchain.py --query \"What are the major differences in pandas 2.0?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the web endpoint:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%bash\n",
    "modal serve modal_langchain.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

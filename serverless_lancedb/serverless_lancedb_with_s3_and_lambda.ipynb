{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serverless LanceDB\n",
    "\n",
    "## Store your data on S3 and use Lambda to compute embeddings and retrieve queries in production easily.\n",
    "<img id=\"splash\" width=\"400\" alt=\"s3-lambda\" src=\"https://user-images.githubusercontent.com/917119/234653050-305a1e90-9305-40ab-b014-c823172a948c.png\">\n",
    "\n",
    "This is a great option if you're wanting to scale with your use case and save effort and costs of maintenance.\n",
    "\n",
    "Let's walk through how to get a simple Lambda function that queries the SIFT dataset on S3.\n",
    "\n",
    "Before we start, you'll need to ensure you create a secure account access to AWS. We recommend using user policies, as this way AWS can share credentials securely without you having to pass around environment variables into Lambda.\n",
    "\n",
    "We'll also use a container to ship our Lambda code. This is a good option for Lambda as you don't have the space limits that you would otherwise by building a package yourself."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup: creating a LanceDB Table and storing it remotely on S3\n",
    "\n",
    "We'll use the SIFT vector dataset as an example. To make it easier, we've already made a Lance-format SIFT dataset publicly available, which we can access and use to populate our LanceDB Table. \n",
    "\n",
    "To do this, download the Lance files locally first from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "s3://eto-public/datasets/sift/vec_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can write a quick Python script to populate our LanceDB Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylance\n",
    "sift_dataset = pylance.dataset(\"/path/to/local/vec_data.lance\")\n",
    "df = sift_dataset.to_table().to_pandas()\n",
    "\n",
    "import lancedb\n",
    "db = lancedb.connect(\".\")\n",
    "table = db.create_table(\"vector_example\", df).lance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've created our Table, we are free to move this data over to S3 so we can remotely host it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building our Lambda app: a simple event handler for vector search\n",
    "\n",
    "Now that we've got a remotely hosted LanceDB Table, we'll want to be able to query it from Lambda. To do so, let's create a new `Dockerfile` using the AWS python container base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "dockerfile"
    }
   },
   "outputs": [],
   "source": [
    "FROM public.ecr.aws/lambda/python:3.10\n",
    "\n",
    "RUN pip3 install --upgrade pip\n",
    "RUN pip3 install --no-cache-dir -U numpy --target \"${LAMBDA_TASK_ROOT}\"\n",
    "RUN pip3 install --no-cache-dir -U lancedb --target \"${LAMBDA_TASK_ROOT}\"\n",
    "\n",
    "COPY app.py ${LAMBDA_TASK_ROOT}\n",
    "\n",
    "CMD [ \"app.handler\" ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a simple Lambda function that queries the SIFT dataset in `app.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import lancedb\n",
    "\n",
    "db = lancedb.connect(\"s3://eto-public/tables\")\n",
    "table = db.open_table(\"vector_example\")\n",
    "\n",
    "def handler(event, context):\n",
    "    status_code = 200\n",
    "\n",
    "    if event['query_vector'] is None:\n",
    "        status_code = 404\n",
    "        return {\n",
    "            \"statusCode\": status_code,\n",
    "            \"headers\": {\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            \"body\": json.dumps({\n",
    "                \"Error \": \"No vector to query was issued\"\n",
    "            })\n",
    "        }\n",
    "    \n",
    "    # Shape of SIFT is (128,1M), d=float32\n",
    "    query_vector = np.array(event['query_vector'], dtype=np.float32)\n",
    "\n",
    "    rs = table.search(query_vector).limit(2).to_df()\n",
    "\n",
    "    return {\n",
    "        \"statusCode\": status_code,\n",
    "        \"headers\": {\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        },\n",
    "        \"body\": rs.to_json()\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying the container to ECR\n",
    "\n",
    "The next step is to build and push the container to ECR, where it can then be used to create a new Lambda function. \n",
    "\n",
    "It's best to follow the official AWS documentation for how to do this, which you can view here:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.aws.amazon.com/lambda/latest/dg/images-create.html#images-upload"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final step: setting up your Lambda function\n",
    "\n",
    "Once the container is pushed, you can create a Lambda function by selecting the container. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
